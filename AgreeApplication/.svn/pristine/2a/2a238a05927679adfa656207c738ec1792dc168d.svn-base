package com.nspl.app.web.rest;

import com.codahale.metrics.annotation.Timed;
import com.nspl.app.domain.DataViews;
import com.nspl.app.domain.DataViewsColumns;
import com.nspl.app.domain.FileTemplateLines;
import com.nspl.app.domain.LookUpCode;
import com.nspl.app.domain.ReportDefination;
import com.nspl.app.domain.ReportParameters;
import com.nspl.app.domain.ReportType;
import com.nspl.app.domain.Reports;
import com.nspl.app.repository.DataViewsColumnsRepository;
import com.nspl.app.repository.DataViewsRepository;
import com.nspl.app.repository.FileTemplateLinesRepository;
import com.nspl.app.repository.LookUpCodeRepository;
import com.nspl.app.repository.ReportDefinationRepository;
import com.nspl.app.repository.ReportParametersRepository;
import com.nspl.app.repository.ReportTypeRepository;
import com.nspl.app.repository.ReportsRepository;
import com.nspl.app.service.DataViewsService;
import com.nspl.app.service.PropertiesUtilService;
import com.nspl.app.service.SourceConnectionDetailsService;
import com.nspl.app.web.rest.util.HeaderUtil;
import com.nspl.app.web.rest.util.PaginationUtil;

import io.swagger.annotations.ApiParam;
import io.github.jhipster.web.util.ResponseUtil;

import org.apache.commons.collections4.MapUtils;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.AnalysisException;
import org.apache.spark.sql.Column;
import org.apache.spark.sql.DataFrameReader;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.RelationalGroupedDataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SQLContext;
import org.joda.time.LocalDate;
import org.joda.time.format.DateTimeFormat;
import org.joda.time.format.DateTimeFormatter;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.boot.actuate.endpoint.AutoConfigurationReportEndpoint.Report;
import org.springframework.core.SpringProperties;
import org.springframework.core.env.Environment;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.http.HttpHeaders;
import org.springframework.http.HttpStatus;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.net.URI;
import java.net.URISyntaxException;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.ResultSetMetaData;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Optional;
import java.util.Properties;
import java.util.Set;

import javax.inject.Inject;

/**
 * REST controller for managing Reports.
 */
@RestController
@RequestMapping("/api")
public class ReportsResource {

    private final Logger log = LoggerFactory.getLogger(ReportsResource.class);

    private static final String ENTITY_NAME = "reports";
    
    private final ReportsRepository reportsRepository;
    
    @Inject
    ReportDefinationRepository reportDefinationRepository;
    
    @Inject
    DataViewsRepository dataViewsRepository;
    
    @Inject
    PropertiesUtilService propertiesUtilService;
    
    @Inject
    DataViewsService dataViewsService;
    
    @Inject
    FileTemplateLinesRepository fileTemplateLinesRepository;
    
    @Inject
    DataViewsColumnsRepository dataViewsColumnsRepository;
    
    @Inject
    ReportParametersRepository reportParametersRepository;

    @Inject
    ReportTypeRepository reportTypeRepository;
    
    @Inject
    LookUpCodeRepository lookUpCodeRepository;
    
    @Inject
    private Environment env;
    
    public ReportsResource(ReportsRepository reportsRepository) {
        this.reportsRepository = reportsRepository;
    }

    /**
     * POST  /reports : Create a new reports.
     *
     * @param reports the reports to create
     * @return the ResponseEntity with status 201 (Created) and with body the new reports, or with status 400 (Bad Request) if the reports has already an ID
     * @throws URISyntaxException if the Location URI syntax is incorrect
     */
    @PostMapping("/reports")
    @Timed
    public ResponseEntity<Reports> createReports(@RequestBody Reports reports) throws URISyntaxException {
        log.debug("REST request to save Reports : {}", reports);
        if (reports.getId() != null) {
            return ResponseEntity.badRequest().headers(HeaderUtil.createFailureAlert(ENTITY_NAME, "idexists", "A new reports cannot already have an ID")).body(null);
        }
        Reports result = reportsRepository.save(reports);
        return ResponseEntity.created(new URI("/api/reports/" + result.getId()))
            .headers(HeaderUtil.createEntityCreationAlert(ENTITY_NAME, result.getId().toString()))
            .body(result);
    }

    /**
     * PUT  /reports : Updates an existing reports.
     *
     * @param reports the reports to update
     * @return the ResponseEntity with status 200 (OK) and with body the updated reports,
     * or with status 400 (Bad Request) if the reports is not valid,
     * or with status 500 (Internal Server Error) if the reports couldn't be updated
     * @throws URISyntaxException if the Location URI syntax is incorrect
     */
    @PutMapping("/reports")
    @Timed
    public ResponseEntity<Reports> updateReports(@RequestBody Reports reports) throws URISyntaxException {
        log.debug("REST request to update Reports : {}", reports);
        if (reports.getId() == null) {
            return createReports(reports);
        }
        Reports result = reportsRepository.save(reports);
        return ResponseEntity.ok()
            .headers(HeaderUtil.createEntityUpdateAlert(ENTITY_NAME, reports.getId().toString()))
            .body(result);
    }

    /**
     * GET  /reports : get all the reports.
     *
     * @param pageable the pagination information
     * @return the ResponseEntity with status 200 (OK) and the list of reports in body
     */
    @GetMapping("/reports")
    @Timed
    public ResponseEntity<List<Reports>> getAllReports(@ApiParam Pageable pageable) {
        log.debug("REST request to get a page of Reports");
        Page<Reports> page = reportsRepository.findAll(pageable);
        HttpHeaders headers = PaginationUtil.generatePaginationHttpHeaders(page, "/api/reports");
        return new ResponseEntity<>(page.getContent(), headers, HttpStatus.OK);
    }

    /**
     * GET  /reports/:id : get the "id" reports.
     *
     * @param id the id of the reports to retrieve
     * @return the ResponseEntity with status 200 (OK) and with body the reports, or with status 404 (Not Found)
     */
    @GetMapping("/reports/{id}")
    @Timed
    public ResponseEntity<Reports> getReports(@PathVariable Long id) {
        log.debug("REST request to get Reports : {}", id);
        Reports reports = reportsRepository.findOne(id);
        return ResponseUtil.wrapOrNotFound(Optional.ofNullable(reports));
    }

    /**
     * DELETE  /reports/:id : delete the "id" reports.
     *
     * @param id the id of the reports to delete
     * @return the ResponseEntity with status 200 (OK)
     */
    @DeleteMapping("/reports/{id}")
    @Timed
    public ResponseEntity<Void> deleteReports(@PathVariable Long id) {
        log.debug("REST request to delete Reports : {}", id);
        reportsRepository.delete(id);
        return ResponseEntity.ok().headers(HeaderUtil.createEntityDeletionAlert(ENTITY_NAME, id.toString())).build();
    }
    
    /**
     * Author: Swetha
     * GET: /getReportsByTenant - Api to fetch list of Reports tagged for a Tenant
     * @param tenantId
     * @return
     */
    @GetMapping("/getReportsByTenant")
    @Timed
    public List<Reports> getReportsByTenant(@RequestParam Long tenantId){
    	log.debug("REST request to getReportsByTenant by tenantId: "+tenantId);
    	List<Reports> reportsList=reportsRepository.fectchActiveReportsByTenantId(tenantId);
		return reportsList;
    	
    }
    
    /**
     * Author: Swetha
     * Api to validate duplication of ReportName
     * @param tenantId
     * @param reportName
     * @return
     */
    @GetMapping("/validateReportName")
    @Timed
    public Long validateReportName(@RequestParam Long tenantId, @RequestParam String reportName){
    	
    	log.info("Rest Request to validateReportName with tenantId: "+tenantId+" and reportName: "+reportName);
    	Long result=0l;
    	Long count=reportsRepository.fetchReportNameCount(tenantId, reportName);
    	log.info("count of reportName: "+reportName+" is: "+count);
    	if(count>1){
    		
    	}
    	else if(count==1){
    		Reports reports=reportsRepository.findByTenantIdAndReportName(tenantId, reportName);
    		result=reports.getId();
    	}
		return result;
    }
    
    /**
     * Author: Swetha
     * Api for Report Generation using Spark Sql
     * @param reportId
     * @param tenanatId
     * @param filtersMap
     * @return
     * @throws ClassNotFoundException
     */
    @PostMapping("/generateReport")
    @Timed
    public List<LinkedHashMap> generateReport(@RequestParam Long reportId, @RequestParam Long tenanatId, @RequestBody(required=false) HashMap filtersMap){
    	
    	log.info("Rest Request to retrieve generateReport3 for reportId: "+reportId+" and tenanatId: "+tenanatId);
    	
    	Reports reports=reportsRepository.findOne(reportId);
    	Long tenantId=reports.getTenantId();
    	Long repTypeId=reports.getReportTypeId();
    	ReportType repType=reportTypeRepository.findOne(repTypeId);
    	String tableName="";
    	String repTypeName="";
    	if(repType!=null){
    		repTypeName=repType.getType();
    		if(repTypeName.equalsIgnoreCase("ACCOUNT_ANALYSIS_REPORT")){
    			tableName="t_account_analysis";
    		}
    		else if(repTypeName.equalsIgnoreCase("ACCOUNT_BALANCE_REPORT")){
    			tableName="t_balance_type";
    		}
    	}
    	
    	/* Report Defination Columns */
    	List<ReportDefination> repDefList=reportDefinationRepository.findByReportId(reportId);
    	log.info("repDefList: "+repDefList);
        
    	
    	LinkedHashMap fieldRefMap=dataViewsService.getFieldRef(reportId, tableName);
    	log.info("fieldRefMap: "+fieldRefMap);
    	
    	LinkedHashMap<String, String> reversedHashMap = new LinkedHashMap<String, String>();
    	Set<String> keySet=fieldRefMap.keySet();
		for (String key : keySet){
		    reversedHashMap.put(fieldRefMap.get(key).toString(), key);
		}
    	
    	 SparkConf sConf =  new SparkConf().setAppName("Reports").setMaster("yarn-cluster").setMaster("local[*]");
         JavaSparkContext sContext =  new JavaSparkContext(sConf);
         SQLContext sqlCont = new SQLContext(sContext);
         
         long startTime = System.currentTimeMillis();   //gives the start time of a process
         long pivotstartTime = System.currentTimeMillis();   //gives the start time of a pivot
         
    //     Properties props = propertiesUtilService.getPropertiesFromClasspath("File.properties");
     	
    // 	String jdbcDriver = props.getProperty("jdbcDriver");  
    //    String schemaName = props.getProperty("schemaName");  
    // 	String dbUrl = props.getProperty("dbUrl")+schemaName;  
     	
    // 	   String userName = props.getProperty("userName"); 
    // 	   String password = props.getProperty("password"); 
         
 		String dbUrl=env.getProperty("spring.datasource.url");
 		String[] parts=dbUrl.split("[\\s@&?$+-]+");
 		String host = parts[0].split("/")[2].split(":")[0];
 		String schemaName=parts[0].split("/")[3];
 		String userName = env.getProperty("spring.datasource.username");
 		String password = env.getProperty("spring.datasource.password");
 		String jdbcDriver = env.getProperty("spring.datasource.jdbcdriver");
        
         Dataset<Row> reports_data = sqlCont.read().format("jdbc")
                 .option("url", dbUrl)
                 .option("user",userName)
                 .option("password",password)
                 .option("dbtable",tableName).load();
         
        log.info("reports_data.count(): "+reports_data.count());
       
        reports_data.show();
        
        String filter="report_id = "+reportId;
        String finQuery="";
        
        log.info("filtersMap: "+filtersMap);
		if(filtersMap!=null){
		
			String conQuery=" where ";
	        String conSubQueryFin="";
			if(filtersMap!=null){
				log.info("filters exists");
	        	if(filtersMap.containsKey("fields")){
	        		List<HashMap> filtersList=(List<HashMap>) filtersMap.get("fields");
	        		log.info("fields exists with sz: "+filtersList.size());
	        		for(int i=0;i<filtersList.size();i++){
	        			HashMap filterMap=filtersList.get(i);
	        			log.info("filterMap: "+filterMap);
	        			String conSubQuery="";
	        			String colName=filterMap.get("fieldName").toString();
	        			log.info("colName: "+colName);
	        			if(filterMap.get("refType").toString().equalsIgnoreCase("FIN_FUNCTION")){
	        				Long refColId=Long.parseLong(filterMap.get("refColId").toString());
	        				LookUpCode finFunCode=lookUpCodeRepository.findOne(refColId);
	        				log.info("finFunCode: "+finFunCode);
	        				colName=finFunCode.getLookUpCode().toLowerCase();
	        				//log.info("colName from lookups: "+colName);
	        			}
	        			else {
	        				//log.info("colName in else b4: "+colName);
	        				colName=fieldRefMap.get(colName).toString();
	        				//log.info("colName in else after: "+colName);
	        			}
	        			String selType=filterMap.get("fieldType").toString();
	        			log.info("selType: "+selType);
	        			if(selType.equalsIgnoreCase("MULTI_SELECT_LOV") || selType.equalsIgnoreCase("SINGLE_SELECT_LOV")  || selType.equalsIgnoreCase("SINGLE_SELECTION") || selType.equalsIgnoreCase("TEXT")){
	        				String selValData="";
	        				String selValVar=filterMap.get("selectedValue").toString();
	        				log.info("selValVar: "+selValVar);
	        				selValVar=selValVar.replace("[", "");
	        				selValVar=selValVar.replace("]", "");
	        				List<String> selValList = new ArrayList<String>(Arrays.asList(selValVar.split(",")));
	        				for(int k=0;k<selValList.size();k++){
	        					String selVal=selValList.get(k);
	        					selVal=selVal.trim();
	        					selValData=selValData+"'"+selVal+"'";
	        					if(k>=0 && k<selValList.size()-1){
	        						selValData=selValData+",";
	        					}
	        				}
	        				if(selValData!=null && !(selValData.isEmpty())){
	        					if(selValData.endsWith(",")){
	        						int indx=selValData.lastIndexOf(",");
	        						selValData=selValData.substring(0,indx);
	        						log.info("selValData after removing comma: "+selValData);
	        					}
	        					conSubQuery=conSubQuery+colName+" in ("+selValData+") ";
	        					log.info("conSubQuery: "+conSubQuery);
	        				}
	        			}
	        			else if(selType.equalsIgnoreCase("AUTO_COMPLETE")){
	        				String selVal=filterMap.get("selectedValue").toString();
	        				if(selVal!=null && !(selVal.isEmpty())){
		        				conSubQuery=conSubQuery+colName+" in ('"+selVal+"') ";
		        				}
	        			}
	        			else if(selType.equalsIgnoreCase("BOOLEAN_SELECTION")){
	        				String selVal=filterMap.get("selectedValue").toString();
	        				if(selVal!=null && !(selVal.isEmpty())){
	        					if(conSubQuery.equalsIgnoreCase(" where ")){
	        	        			
	        	        		}
	        					else conSubQuery=" and ";
	        				conSubQuery=conSubQuery+colName+" is "+selVal;
	        				}
	        			}
	        			else if(selType.equalsIgnoreCase("AMOUNT_RANGE")){
	        				String map=filterMap.get("selectedValue").toString();
	        				if(map!=null && !(map.isEmpty())){
	        				log.info("map: "+map);
	        				//String map="selectedValue={fromValue=1036873.4, toValue=1377186}";
	        				map=map.replace("{fromValue=", ""); //1036873.4, toValue=1377186}"
	        				map=map.replace("toValue=", ""); //1036873.4,1377186}
	        				log.info("map aftr replace: "+map);
	        				String arr[]=map.split(",");
	        				log.info(" arr[0]: "+arr[0]);
	        				log.info("arr[1]: "+arr[1]);
	        				arr[1]=arr[1].replace("}", "");
	        				System.out.println("arr[1]: "+arr[1]+" arr[0]: "+arr[0]);
	        				String fromValue=arr[0].trim();
	        				String toValue=arr[1].trim();
	        				conSubQuery=conSubQuery+colName+" between "+fromValue+" and "+toValue;	
	        				}
	        			}
	        		log.info("conSubQuery: "+conSubQuery);
	        		if(conSubQuery.length()>1){
	        			conSubQuery=conSubQuery+" and ";
	        		}
	        		conSubQueryFin=conSubQuery;
	        		if(conSubQuery.equalsIgnoreCase(" where ")){
	        			
	        		}
	        		else{
	        			finQuery=finQuery+conSubQuery;
	        		}
	        			log.info("finQuery after conQuery: "+finQuery);
	        		}
	        		
	        	}
	        }
			else{
				log.info("filters doesnt exist");
			}
	        log.info("finQuery after filters: "+finQuery);
	        log.info("conSubQueryFin: "+conSubQueryFin);
		}
		else{
			
		}
		if(finQuery.endsWith("and ")){
        	int indx=finQuery.lastIndexOf("and");
        	finQuery=finQuery.substring(0,indx);
        }
		if(finQuery!=null && !(finQuery.isEmpty())){
			
			finQuery=finQuery+" and "+filter;
		}
		else {

			log.info("filter: "+filter);
			finQuery=filter;
			log.info("finQuery:" +finQuery);
		}
        
        log.info("finQuery: "+finQuery);
        Dataset<Row> dt = reports_data.filter(finQuery);
    
        List<Row> data=dt.collectAsList();
        String[] columnsList=dt.columns();
        
        log.info("data.size(): "+data.size()+" columnsList.length: "+columnsList.length);
        List<LinkedHashMap> maps=new ArrayList<LinkedHashMap>();
        for(int j=0;j<data.size();j++){
        	LinkedHashMap map=new LinkedHashMap();
            for(int s=0;s<(columnsList.length);s++){
            	String tabColName=columnsList[s];
            	log.info("tabColName: "+tabColName);
            	log.info("reversedHashMap: "+reversedHashMap);
            	if(data.get(j).get(s)!=null && !(data.get(j).get(s).toString().isEmpty())){
            		log.info("reversedHashMap.containsKey(tabColName): "+reversedHashMap.containsKey(tabColName));
            	if(reversedHashMap.containsKey(tabColName) && reversedHashMap.get(tabColName)!=null){
            		log.info("reversedHashMap.get(tabColName): "+reversedHashMap.get(tabColName));
            		String val=reversedHashMap.get(tabColName).toString();
            		log.info("val: "+val);
            		tabColName=val;
            	}
            	else{
            		log.info("repTypeName: "+repTypeName);
            		LookUpCode lCode=lookUpCodeRepository.findByLookUpTypeAndLookUpCodeAndTenantId(repTypeName,tabColName.toUpperCase() , tenantId);
            		log.info("lCode: "+lCode);
            		if(lCode!=null){
            			Long lId=lCode.getId();
            			ReportDefination repDef=reportDefinationRepository.findByReportIdAndRefTypeIdAndRefColId(reportId,"FIN_FUNCTION",lId);
            			if(repDef!=null)
            				tabColName=repDef.getDisplayName();
            		}
            	}
            	
            	log.info("final tabColName: "+tabColName);
            	for(int p=0;p<repDefList.size();p++){
            	if(repDefList.get(p).getDisplayName().equalsIgnoreCase(tabColName)){
            		map.put(tabColName,data.get(j).get(s));
            	}
            	else{
            		
            	}
            	}
             	
             	 log.info("map b47 adding: "+map);
            	}
            }
            maps.add(map);
        }
                sContext.close();
               log.info("Total Time taken: "+(System.currentTimeMillis()-startTime)+"ms");  //gives the overall time taken by the proces
                return maps; 
        
    }

    
    @PostMapping("/generateReport2")
    @Timed
    public List<LinkedHashMap> generateReport2(@RequestParam Long reportId, @RequestParam Long tenanatId, @RequestBody(required=false) HashMap filtersMap){
    	
    	log.info("Rest Request to retrieve generateReport3 for reportId: "+reportId+" and tenanatId: "+tenanatId);
    	
    	Reports reports=reportsRepository.findOne(reportId);
    	Long tenantId=reports.getTenantId();
    	Long repTypeId=reports.getReportTypeId();
    	ReportType repType=reportTypeRepository.findOne(repTypeId);
    	String tableName="";
    	String repTypeName="";
    	if(repType!=null){
    		repTypeName=repType.getType();
    		if(repTypeName.equalsIgnoreCase("ACCOUNT_ANALYSIS_REPORT")){
    			tableName="t_account_analysis";
    		}
    		else if(repTypeName.equalsIgnoreCase("ACCOUNT_BALANCE_REPORT")){
    			tableName="t_balance_type";
    		}
    	}
    	
    	/* Report Defination Columns */
    	List<ReportDefination> repDefList=reportDefinationRepository.findByReportId(reportId);
    	log.info("repDefList: "+repDefList);
        
    	
    	LinkedHashMap fieldRefMap=dataViewsService.getFieldRef(reportId, tableName);
    	log.info("fieldRefMap: "+fieldRefMap);
    	
    	LinkedHashMap<String, String> reversedHashMap = new LinkedHashMap<String, String>();
    	Set<String> keySet=fieldRefMap.keySet();
		for (String key : keySet){
		    reversedHashMap.put(fieldRefMap.get(key).toString(), key);
		}
    	
    	 SparkConf sConf =  new SparkConf().setAppName("Reports").setMaster("yarn-cluster").setMaster("local[*]");
         JavaSparkContext sContext =  new JavaSparkContext(sConf);
         SQLContext sqlCont = new SQLContext(sContext);
         
         long startTime = System.currentTimeMillis();   //gives the start time of a process
         long pivotstartTime = System.currentTimeMillis();   //gives the start time of a pivot
         
     //    Properties props = propertiesUtilService.getPropertiesFromClasspath("File.properties");
     	
    // 	String jdbcDriver = props.getProperty("jdbcDriver");  
    //    String schemaName = props.getProperty("schemaName");  
    // 	String dbUrl = props.getProperty("dbUrl")+schemaName;  
     	
    // 	   String userName = props.getProperty("userName"); 
    // 	   String password = props.getProperty("password"); 
         
 		String dbUrl=env.getProperty("spring.datasource.url");
 		String[] parts=dbUrl.split("[\\s@&?$+-]+");
 		String host = parts[0].split("/")[2].split(":")[0];
 		String schemaName=parts[0].split("/")[3];
 		String userName = env.getProperty("spring.datasource.username");
 		String password = env.getProperty("spring.datasource.password");
 		String jdbcDriver = env.getProperty("spring.datasource.jdbcdriver");
        
         Dataset<Row> reports_data = sqlCont.read().format("jdbc")
                 .option("url", dbUrl)
                 .option("user",userName)
                 .option("password",password)
                 .option("dbtable",tableName).load();
         
        log.info("reports_data.count(): "+reports_data.count());
       
        reports_data.show();
        
        String filter="report_id = "+reportId;
        String finQuery="";
        
        log.info("filtersMap: "+filtersMap);
		if(filtersMap!=null){
		
			String conQuery=" where ";
	        String conSubQueryFin="";
			if(filtersMap!=null){
				log.info("filters exists");
	        	if(filtersMap.containsKey("fields")){
	        		List<HashMap> filtersList=(List<HashMap>) filtersMap.get("fields");
	        		log.info("fields exists with sz: "+filtersList.size());
	        		for(int i=0;i<filtersList.size();i++){
	        			HashMap filterMap=filtersList.get(i);
	        			log.info("filterMap: "+filterMap);
	        			String conSubQuery="";
	        			String colName=filterMap.get("fieldName").toString();
	        			log.info("colName: "+colName);
	        			if(filterMap.get("refType").toString().equalsIgnoreCase("FIN_FUNCTION")){
	        				Long refColId=Long.parseLong(filterMap.get("refColId").toString());
	        				LookUpCode finFunCode=lookUpCodeRepository.findOne(refColId);
	        				log.info("finFunCode: "+finFunCode);
	        				colName=finFunCode.getLookUpCode().toLowerCase();
	        				//log.info("colName from lookups: "+colName);
	        			}
	        			else {
	        				//log.info("colName in else b4: "+colName);
	        				colName=fieldRefMap.get(colName).toString();
	        				//log.info("colName in else after: "+colName);
	        			}
	        			String selType=filterMap.get("fieldType").toString();
	        			log.info("selType: "+selType);
	        			if(selType.equalsIgnoreCase("MULTI_SELECT_LOV") || selType.equalsIgnoreCase("SINGLE_SELECT_LOV")  || selType.equalsIgnoreCase("SINGLE_SELECTION") || selType.equalsIgnoreCase("TEXT")){
	        				String selValData="";
	        				String selValVar=filterMap.get("selectedValue").toString();
	        				log.info("selValVar: "+selValVar);
	        				selValVar=selValVar.replace("[", "");
	        				selValVar=selValVar.replace("]", "");
	        				List<String> selValList = new ArrayList<String>(Arrays.asList(selValVar.split(",")));
	        				for(int k=0;k<selValList.size();k++){
	        					String selVal=selValList.get(k);
	        					selVal=selVal.trim();
	        					selValData=selValData+"'"+selVal+"'";
	        					if(k>=0 && k<selValList.size()-1){
	        						selValData=selValData+",";
	        					}
	        				}
	        				if(selValData!=null && !(selValData.isEmpty())){
	        					if(selValData.endsWith(",")){
	        						int indx=selValData.lastIndexOf(",");
	        						selValData=selValData.substring(0,indx);
	        						log.info("selValData after removing comma: "+selValData);
	        					}
	        					conSubQuery=conSubQuery+colName+" in ("+selValData+") ";
	        					log.info("conSubQuery: "+conSubQuery);
	        				}
	        			}
	        			else if(selType.equalsIgnoreCase("AUTO_COMPLETE")){
	        				String selVal=filterMap.get("selectedValue").toString();
	        				if(selVal!=null && !(selVal.isEmpty())){
		        				conSubQuery=conSubQuery+colName+" in ('"+selVal+"') ";
		        				}
	        			}
	        			else if(selType.equalsIgnoreCase("BOOLEAN_SELECTION")){
	        				String selVal=filterMap.get("selectedValue").toString();
	        				if(selVal!=null && !(selVal.isEmpty())){
	        					if(conSubQuery.equalsIgnoreCase(" where ")){
	        	        			
	        	        		}
	        					else conSubQuery=" and ";
	        				conSubQuery=conSubQuery+colName+" is "+selVal;
	        				}
	        			}
	        			else if(selType.equalsIgnoreCase("AMOUNT_RANGE")){
	        				String map=filterMap.get("selectedValue").toString();
	        				if(map!=null && !(map.isEmpty())){
	        				log.info("map: "+map);
	        				//String map="selectedValue={fromValue=1036873.4, toValue=1377186}";
	        				map=map.replace("{fromValue=", ""); //1036873.4, toValue=1377186}"
	        				map=map.replace("toValue=", ""); //1036873.4,1377186}
	        				log.info("map aftr replace: "+map);
	        				String arr[]=map.split(",");
	        				log.info(" arr[0]: "+arr[0]);
	        				log.info("arr[1]: "+arr[1]);
	        				arr[1]=arr[1].replace("}", "");
	        				System.out.println("arr[1]: "+arr[1]+" arr[0]: "+arr[0]);
	        				String fromValue=arr[0].trim();
	        				String toValue=arr[1].trim();
	        				conSubQuery=conSubQuery+colName+" between "+fromValue+" and "+toValue;	
	        				}
	        			}
	        		log.info("conSubQuery: "+conSubQuery);
	        		if(conSubQuery.length()>1){
	        			conSubQuery=conSubQuery+" and ";
	        		}
	        		conSubQueryFin=conSubQuery;
	        		if(conSubQuery.equalsIgnoreCase(" where ")){
	        			
	        		}
	        		else{
	        			finQuery=finQuery+conSubQuery;
	        		}
	        			log.info("finQuery after conQuery: "+finQuery);
	        		}
	        		
	        	}
	        }
			else{
				log.info("filters doesnt exist");
			}
	        log.info("finQuery after filters: "+finQuery);
	        log.info("conSubQueryFin: "+conSubQueryFin);
		}
		else{
			
		}
		if(finQuery.endsWith("and ")){
        	int indx=finQuery.lastIndexOf("and");
        	finQuery=finQuery.substring(0,indx);
        }
		if(finQuery!=null && !(finQuery.isEmpty())){
			
			finQuery=finQuery+" and "+filter;
		}
		else {

			log.info("filter: "+filter);
			finQuery=filter;
			log.info("finQuery:" +finQuery);
		}
        
        log.info("finQuery: "+finQuery);
		return null;
        
       /* String Query="SELECT record_type, processor_name , currency_code,transaction_date, SUM(amount) AS amount FROM recon_data1 WHERE transaction_date <= current_date() - INTERVAL 30 day  and transaction_date between '"+startDate+"' and '"+endDate+"' GROUP BY record_type, processor_name , currency_code,transaction_date";
        log.info("Query: "+Query);
        sqlCont.sql("")*/
        //sqlCont().sql(weekWiseQuery);
    }


    @PostMapping("/testSql")
    @Timed
    public List<LinkedHashMap> testSql(@RequestParam Long reportId, @RequestParam Long tenanatId, @RequestBody(required=false) HashMap filtersMap){
    	
    	log.info("Rest Request to retrieve generateReport3 for reportId: "+reportId+" and tenanatId: "+tenanatId);
    	
    	Reports reports=reportsRepository.findOne(reportId);
    	Long tenantId=reports.getTenantId();
    	Long repTypeId=reports.getReportTypeId();
    	ReportType repType=reportTypeRepository.findOne(repTypeId);
    	String tableName="";
    	String repTypeName="";
    	if(repType!=null){
    		repTypeName=repType.getType();
    		if(repTypeName.equalsIgnoreCase("ACCOUNT_ANALYSIS_REPORT")){
    			tableName="t_account_analysis";
    		}
    		else if(repTypeName.equalsIgnoreCase("ACCOUNT_BALANCE_REPORT")){
    			tableName="t_balance_type";
    		}
    	}
    	
    	/* Report Defination Columns */
    	List<ReportDefination> repDefList=reportDefinationRepository.findByReportId(reportId);
    	log.info("repDefList: "+repDefList);
        
    	 SparkConf sConf =  new SparkConf().setAppName("Reports").setMaster("yarn-cluster").setMaster("local[*]");
         JavaSparkContext sContext =  new JavaSparkContext(sConf);
         SQLContext sqlCont = new SQLContext(sContext);
         
         long startTime = System.currentTimeMillis();   //gives the start time of a process
         long pivotstartTime = System.currentTimeMillis();   //gives the start time of a pivot
         
    //     Properties props = propertiesUtilService.getPropertiesFromClasspath("File.properties");
     	
   //  	String jdbcDriver = props.getProperty("jdbcDriver");  
    //    String schemaName = props.getProperty("schemaName");  
   //  	String dbUrl = props.getProperty("dbUrl")+schemaName;  
     	
    // 	   String userName = props.getProperty("userName"); 
     //	   String password = props.getProperty("password"); 
         
  		String dbUrl=env.getProperty("spring.datasource.url");
  		String[] parts=dbUrl.split("[\\s@&?$+-]+");
  		String host = parts[0].split("/")[2].split(":")[0];
  		String schemaName=parts[0].split("/")[3];
  		String userName = env.getProperty("spring.datasource.username");
  		String password = env.getProperty("spring.datasource.password");
  		String jdbcDriver = env.getProperty("spring.datasource.jdbcdriver");
        
     	  DataFrameReader reports_data = sqlCont.read().format("jdbc")
                 .option("url", dbUrl)
                 .option("user",userName)
                 .option("password",password)
                 .option("dbtable","t_reports");
         
         Dataset<Row> data1 = reports_data.load();
         
         data1.registerTempTable("reports_data");
         
         log.info("reports_data.count(): ");
         
         //log.info("reports_data.count(): "+reports_data.count());
         
         DataFrameReader reports_data1 = sqlCont.read().format("jdbc")
                 .option("url", dbUrl)
                 .option("user",userName)
                 .option("password",password)
                 .option("dbtable","t_report_type");
         
         Dataset<Row> data2 = reports_data1.load();
         
         data2.registerTempTable("reports_data1");
         
       // log.info("reports_data.count()1: "+reports_data1.count());
       
        //reports_data.show();
        
        String Query="select ds1.type, ds1.allow_drill_down,ds0.id, ds0.report_name, ds0.source_view_id  from (t_reports ) as ds0,"+
        			 "(t_report_type)  as ds1 where  ds1.id=ds0.report_type_id";
        log.info("Query: "+Query);
        Dataset<Row> finData=sqlCont.sql(Query);
        log.info("data sz after sql finData: "+finData);
        //sqlCont().sql(weekWiseQuery);
        sContext.close();
		return null;
    }
    
    
    @PostMapping("/agingReportTest")
    @Timed
    public void agingReportTest(@RequestParam(value="listColumn", required = false) String listColumn, @RequestParam(value="type", required = false) String type, @RequestParam String pivotCol,  @RequestParam(value = "filerColumn", required = false) String filerColumn, @RequestParam(value = "filterValues", required = false) String filterValues) throws AnalysisException{
    SparkConf sConf =  new SparkConf().setAppName("Pivot").setMaster("yarn-cluster").setMaster("local[*]");
    
    sConf.set("spark.driver.allowMultipleContexts", "true");
    
      JavaSparkContext sContext =  new JavaSparkContext(sConf);
       SQLContext sqlCont = new SQLContext(sContext);
    
        long startTime = System.currentTimeMillis();   //gives the start time of a process
        long pivotstartTime = System.currentTimeMillis();   //gives the start time of a pivot
       
        DataFrameReader recon_data1 = sqlCont.read().format("jdbc")
                .option("url", "jdbc:mysql://192.168.0.44:3306/reports")
                .option("user","root")
                .option("password","Welcome789$")
                .option("dbtable","reconcile2");
        
        Dataset<Row> recon_data = recon_data1.load();
        
       recon_data.registerTempTable("recon_data1");
       
    
       DateTimeFormatter dtf = DateTimeFormat.forPattern("yyyy-MM-dd");
       LocalDate curDate = dtf.parseLocalDate("2017-07-04");
       LocalDate startDate = dtf.parseLocalDate("2013-01-01");
       LocalDate endDate = dtf.parseLocalDate("2013-03-31");
       log.info("curDate: "+curDate+" startDate: "+startDate+" endDate: "+endDate);
       //String query="SELECT record_type, processor_name , currency_code,transaction_date,day(transaction_date) AS period, SUM(amount) AS amount FROM recon_data1 WHERE transaction_date <='"+ curDate+"' - INTERVAL 1 day  and transaction_date between '"+startDate+"' and '"+endDate+"' GROUP BY record_type, processor_name , currency_code,transaction_date,period";
      // String weekWiseQuery="SELECT record_type, processor_name , currency_code,transaction_date,WEEKOFYEAR(transaction_date) , SUM(amount) AS amount FROM recon_data1 WHERE transaction_date <= current_date() - INTERVAL 1 day  and transaction_date between '"+startDate+"' and '"+endDate+"' GROUP BY record_type, processor_name , currency_code,transaction_date,WEEKOFYEAR(transaction_date)";
      // String weekWiseQuery="SELECT record_type, processor_name , currency_code  FROM recon_data1";
       String weekWiseQuery="SELECT * FROM recon_data1 as ds";
       log.info("weekWiseQuery: "+weekWiseQuery);
       Dataset<Row> weekWisedata = recon_data1.table("recon_data1").sqlContext().sql(weekWiseQuery).sort("record_type");
      // Dataset<Row> weekWisedataPivot=weekWisedata.groupBy("record_type","processor_name").pivot("WEEKOFYEAR(transaction_date)").sum("amount");
       int sz4=weekWisedata.collectAsList().size();
       log.info("sz4: "+sz4);
       //weekWisedata.show(sz4);
   
       weekWisedata.show(100);
       sContext.close();
    }
    
    
    @PostMapping("/agingReportPOC")
    @Timed
    public void agingReportPOC(@RequestParam(value="listColumn", required = false) String listColumn, @RequestParam(value="type", required = false) String type, @RequestParam String pivotCol,  @RequestParam(value = "filerColumn", required = false) String filerColumn, @RequestParam(value = "filterValues", required = false) String filterValues) throws AnalysisException{
    SparkConf sConf =  new SparkConf().setAppName("Pivot").setMaster("yarn-cluster").setMaster("local[*]");
    
    sConf.set("spark.driver.allowMultipleContexts", "true");
    
      JavaSparkContext sContext =  new JavaSparkContext(sConf);
       SQLContext sqlCont = new SQLContext(sContext);
    
        long startTime = System.currentTimeMillis();   //gives the start time of a process
        long pivotstartTime = System.currentTimeMillis();   //gives the start time of a pivot
       
        DataFrameReader recon_data1 = sqlCont.read().format("jdbc")
                .option("url", "jdbc:mysql://192.168.0.44:3306/reports")
                .option("user","root")
                .option("password","Welcome789$")
                .option("dbtable","reconcile2");
        
        Dataset<Row> recon_data = recon_data1.load();
        
       recon_data.registerTempTable("recon_data1");
        
       // recon_data1.table("recon_data1").sqlContext().sql("select * from recon_data1 where record_type='PSP to GL' and processor_name='ADYEN' and currency_code='AED'").show();
       // sql("select * from reconcile2 where record_type='PSP to GL' and processor_name='ADYEN' and currency_code='AED'").show();
      // Dataset<Row> yeardata = recon_data1.table("recon_data1").sqlContext().sql("SELECT Year(transaction_date), Month(transaction_date), Count(*) As Total_Rows FROM recon_data1 GROUP BY Year(transaction_date), Month(transaction_date)");
       
       /* Year wise Pivot */
       /*Dataset<Row> yeardata = recon_data1.table("recon_data1").sqlContext().sql("SELECT record_type, processor_name , currency_code, amount, year(transaction_date), Month(transaction_date), Count(*) As Total_Rows FROM recon_data1 GROUP BY record_type, processor_name , currency_code,amount,Year(transaction_date), Month(transaction_date)").sort("record_type");
       //Dataset<Row> yeardata = recon_data1.table("recon_data1").sqlContext().sql("SELECT record_type, processor_name , currency_code, amount, year(transaction_date), Monthname(transaction_date), Count(*) As Total_Rows FROM recon_data1 GROUP BY record_type, processor_name , currency_code,amount,Year(transaction_date), Month(transaction_date)").sort("record_type");
       Dataset<Row> yeardataPivot=yeardata.groupBy("record_type","processor_name").pivot("year(transaction_date)").sum("amount");
       int sz=yeardataPivot.collectAsList().size();
       yeardataPivot.show(sz);*/
       
       
      /* Dataset<Row> mnthdata = recon_data1.table("recon_data1").sqlContext().sql("SELECT record_type, processor_name , currency_code, amount, year(transaction_date), monthname(transaction_date), Count(*) As Total_Rows FROM recon_data1 GROUP BY record_type, processor_name , currency_code,amount,Year(transaction_date), monthname(transaction_date)").sort("record_type");
       mnthdata.groupBy("record_type","processor_name").pivot("monthname(transaction_date)").sum("amount").show();*/
       
       /* Month wise Pivot */
      /* Dataset<Row> mnthdata = recon_data1.table("recon_data1").sqlContext().sql("SELECT record_type, processor_name , currency_code, amount, year(transaction_date), Month(transaction_date), Count(*) As Total_Rows FROM recon_data1 GROUP BY record_type, processor_name , currency_code,amount,Year(transaction_date), Month(transaction_date)").sort("record_type");
       Dataset<Row> mnthdataPivot=mnthdata.groupBy("record_type","processor_name").pivot("month(transaction_date)").sum("amount");
       int sz1=mnthdataPivot.collectAsList().size();
       mnthdataPivot.show(sz1);*/
       
       /* Day wise Pivot */
       /*DateTimeFormatter dtf = DateTimeFormat.forPattern("yyyy-MM-dd");
       LocalDate curDate = dtf.parseLocalDate("2017-07-04");
       LocalDate startDate = dtf.parseLocalDate("2013-01-01");
       LocalDate endDate = dtf.parseLocalDate("2013-01-10");
       log.info("curDate: "+curDate+" startDate: "+startDate+" endDate: "+endDate);
       //String query="SELECT record_type, processor_name , currency_code,transaction_date,day(transaction_date) AS period, SUM(amount) AS amount FROM recon_data1 WHERE transaction_date <='"+ curDate+"' - INTERVAL 1 day  and transaction_date between '"+startDate+"' and '"+endDate+"' GROUP BY record_type, processor_name , currency_code,transaction_date,period";
       String query="SELECT record_type, processor_name , currency_code,transaction_date,day(transaction_date) , SUM(amount) AS amount FROM recon_data1 WHERE transaction_date <= current_date() - INTERVAL 1 day  and transaction_date between '"+startDate+"' and '"+endDate+"' GROUP BY record_type, processor_name , currency_code,transaction_date,day(transaction_date)";
       log.info("query: "+query);
       Dataset<Row> daydata = recon_data1.table("recon_data1").sqlContext().sql(query).sort("record_type");
       Dataset<Row> daydataPivot=daydata.groupBy("record_type","processor_name").pivot("dayofmonth(transaction_date)").sum("amount");
       int sz3=daydataPivot.collectAsList().size();
       daydataPivot.show(sz3);*/
       
       /* week wise reports */
       /*DateTimeFormatter dtf = DateTimeFormat.forPattern("yyyy-MM-dd");
       LocalDate curDate = dtf.parseLocalDate("2017-07-04");
       LocalDate startDate = dtf.parseLocalDate("2013-01-01");
       LocalDate endDate = dtf.parseLocalDate("2013-03-31");
       log.info("curDate: "+curDate+" startDate: "+startDate+" endDate: "+endDate);
       //String query="SELECT record_type, processor_name , currency_code,transaction_date,day(transaction_date) AS period, SUM(amount) AS amount FROM recon_data1 WHERE transaction_date <='"+ curDate+"' - INTERVAL 1 day  and transaction_date between '"+startDate+"' and '"+endDate+"' GROUP BY record_type, processor_name , currency_code,transaction_date,period";
       String weekWiseQuery="SELECT record_type, processor_name , currency_code,transaction_date,WEEKOFYEAR(transaction_date) , SUM(amount) AS amount FROM recon_data1 WHERE transaction_date <= current_date() - INTERVAL 1 day  and transaction_date between '"+startDate+"' and '"+endDate+"' GROUP BY record_type, processor_name , currency_code,transaction_date,WEEKOFYEAR(transaction_date)";
       log.info("weekWiseQuery: "+weekWiseQuery);
       Dataset<Row> weekWisedata = recon_data1.table("recon_data1").sqlContext().sql(weekWiseQuery).sort("record_type");
       Dataset<Row> weekWisedataPivot=weekWisedata.groupBy("record_type","processor_name").pivot("WEEKOFYEAR(transaction_date)").sum("amount");
       int sz4=weekWisedataPivot.collectAsList().size();
       weekWisedataPivot.show(sz4);*/
       
       DateTimeFormatter dtf = DateTimeFormat.forPattern("yyyy-MM-dd");
       LocalDate curDate = dtf.parseLocalDate("2017-07-04");
       LocalDate startDate = dtf.parseLocalDate("2013-01-01");
       LocalDate endDate = dtf.parseLocalDate("2013-03-31");
       int intreval=30;
       log.info("curDate: "+curDate+" startDate: "+startDate+" endDate: "+endDate);
       //String query="SELECT record_type, processor_name , currency_code,transaction_date,day(transaction_date) AS period, SUM(amount) AS amount FROM recon_data1 WHERE transaction_date <='"+ curDate+"' - INTERVAL 1 day  and transaction_date between '"+startDate+"' and '"+endDate+"' GROUP BY record_type, processor_name , currency_code,transaction_date,period";
       String weekWiseQuery="SELECT record_type, processor_name , currency_code,transaction_date, WEEKOFYEAR(transaction_date), SUM(amount) AS amount FROM recon_data1 WHERE transaction_date <= current_date() - INTERVAL 30 day  and transaction_date between '"+startDate+"' and '"+endDate+"' GROUP BY record_type, processor_name , currency_code,transaction_date";
       log.info("weekWiseQuery: "+weekWiseQuery);
       Dataset<Row> weekWisedata = recon_data1.table("recon_data1").sqlContext().sql(weekWiseQuery).sort("record_type");
       Dataset<Row> weekWisedataPivot=weekWisedata.groupBy("record_type","processor_name").pivot("WEEKOFYEAR(transaction_date)").sum("amount");
       int sz4=weekWisedataPivot.collectAsList().size();
       weekWisedataPivot.show(100);
       
       sContext.close();
       /*Dataset<Row> weekdata = recon_data1.table("recon_data1").sqlContext().sql("SELECT record_type, processor_name , currency_code, amount, year(transaction_date), monthname(transaction_date), Count(*) As Total_Rows FROM recon_data1 GROUP BY record_type, processor_name , currency_code,amount,Year(transaction_date), monthname(transaction_date)").sort("record_type");
       mnthdata.groupBy("record_type","processor_name").pivot("monthname(transaction_date)").sum("amount").show();*/
    }
    
    @PostMapping("/agingReport")
    @Timed
    public void agingReport(@RequestParam(value="listColumn", required = false) String listColumn, @RequestParam(value="type", required = false) String type, @RequestParam String pivotCol,  @RequestParam(value = "filerColumn", required = false) String filerColumn, @RequestParam(value = "filterValues", required = false) String filterValues) throws AnalysisException{
    SparkConf sConf =  new SparkConf().setAppName("Pivot").setMaster("yarn-cluster").setMaster("local[*]");
    
    sConf.set("spark.driver.allowMultipleContexts", "true");
    
      JavaSparkContext sContext =  new JavaSparkContext(sConf);
       SQLContext sqlCont = new SQLContext(sContext);
    
        long startTime = System.currentTimeMillis();   //gives the start time of a process
        long pivotstartTime = System.currentTimeMillis();   //gives the start time of a pivot
       
        DataFrameReader recon_data1 = sqlCont.read().format("jdbc")
                .option("url", "jdbc:mysql://localhost:3306/applicationjagan")
                .option("user","root")
                .option("password","welcome")
                .option("dbtable","testtrimcomma");
        
        Dataset<Row> recon_data = recon_data1.load();
        
       recon_data.registerTempTable("recon_data1");
        
       // recon_data1.table("recon_data1").sqlContext().sql("select * from recon_data1 where record_type='PSP to GL' and processor_name='ADYEN' and currency_code='AED'").show();
       // sql("select * from reconcile2 where record_type='PSP to GL' and processor_name='ADYEN' and currency_code='AED'").show();
      // Dataset<Row> yeardata = recon_data1.table("recon_data1").sqlContext().sql("SELECT Year(transaction_date), Month(transaction_date), Count(*) As Total_Rows FROM recon_data1 GROUP BY Year(transaction_date), Month(transaction_date)");
       
       /* Year wise Pivot */
       /*Dataset<Row> yeardata = recon_data1.table("recon_data1").sqlContext().sql("SELECT record_type, processor_name , currency_code, amount, year(transaction_date), Month(transaction_date), Count(*) As Total_Rows FROM recon_data1 GROUP BY record_type, processor_name , currency_code,amount,Year(transaction_date), Month(transaction_date)").sort("record_type");
       //Dataset<Row> yeardata = recon_data1.table("recon_data1").sqlContext().sql("SELECT record_type, processor_name , currency_code, amount, year(transaction_date), Monthname(transaction_date), Count(*) As Total_Rows FROM recon_data1 GROUP BY record_type, processor_name , currency_code,amount,Year(transaction_date), Month(transaction_date)").sort("record_type");
       Dataset<Row> yeardataPivot=yeardata.groupBy("record_type","processor_name").pivot("year(transaction_date)").sum("amount");
       int sz=yeardataPivot.collectAsList().size();
       yeardataPivot.show(sz);*/
       
       
      /* Dataset<Row> mnthdata = recon_data1.table("recon_data1").sqlContext().sql("SELECT record_type, processor_name , currency_code, amount, year(transaction_date), monthname(transaction_date), Count(*) As Total_Rows FROM recon_data1 GROUP BY record_type, processor_name , currency_code,amount,Year(transaction_date), monthname(transaction_date)").sort("record_type");
       mnthdata.groupBy("record_type","processor_name").pivot("monthname(transaction_date)").sum("amount").show();*/
       
       /* Month wise Pivot */
      /* Dataset<Row> mnthdata = recon_data1.table("recon_data1").sqlContext().sql("SELECT record_type, processor_name , currency_code, amount, year(transaction_date), Month(transaction_date), Count(*) As Total_Rows FROM recon_data1 GROUP BY record_type, processor_name , currency_code,amount,Year(transaction_date), Month(transaction_date)").sort("record_type");
       Dataset<Row> mnthdataPivot=mnthdata.groupBy("record_type","processor_name").pivot("month(transaction_date)").sum("amount");
       int sz1=mnthdataPivot.collectAsList().size();
       mnthdataPivot.show(sz1);*/
       
       /* Day wise Pivot */
       /*DateTimeFormatter dtf = DateTimeFormat.forPattern("yyyy-MM-dd");
       LocalDate curDate = dtf.parseLocalDate("2017-07-04");
       LocalDate startDate = dtf.parseLocalDate("2013-01-01");
       LocalDate endDate = dtf.parseLocalDate("2013-01-10");
       log.info("curDate: "+curDate+" startDate: "+startDate+" endDate: "+endDate);
       //String query="SELECT record_type, processor_name , currency_code,transaction_date,day(transaction_date) AS period, SUM(amount) AS amount FROM recon_data1 WHERE transaction_date <='"+ curDate+"' - INTERVAL 1 day  and transaction_date between '"+startDate+"' and '"+endDate+"' GROUP BY record_type, processor_name , currency_code,transaction_date,period";
       String query="SELECT record_type, processor_name , currency_code,transaction_date,day(transaction_date) , SUM(amount) AS amount FROM recon_data1 WHERE transaction_date <= current_date() - INTERVAL 1 day  and transaction_date between '"+startDate+"' and '"+endDate+"' GROUP BY record_type, processor_name , currency_code,transaction_date,day(transaction_date)";
       log.info("query: "+query);
       Dataset<Row> daydata = recon_data1.table("recon_data1").sqlContext().sql(query).sort("record_type");
       Dataset<Row> daydataPivot=daydata.groupBy("record_type","processor_name").pivot("dayofmonth(transaction_date)").sum("amount");
       int sz3=daydataPivot.collectAsList().size();
       daydataPivot.show(sz3);*/
       
       /* week wise reports */
       /*DateTimeFormatter dtf = DateTimeFormat.forPattern("yyyy-MM-dd");
       LocalDate curDate = dtf.parseLocalDate("2017-07-04");
       LocalDate startDate = dtf.parseLocalDate("2013-01-01");
       LocalDate endDate = dtf.parseLocalDate("2013-03-31");
       log.info("curDate: "+curDate+" startDate: "+startDate+" endDate: "+endDate);
       //String query="SELECT record_type, processor_name , currency_code,transaction_date,day(transaction_date) AS period, SUM(amount) AS amount FROM recon_data1 WHERE transaction_date <='"+ curDate+"' - INTERVAL 1 day  and transaction_date between '"+startDate+"' and '"+endDate+"' GROUP BY record_type, processor_name , currency_code,transaction_date,period";
       String weekWiseQuery="SELECT record_type, processor_name , currency_code,transaction_date,WEEKOFYEAR(transaction_date) , SUM(amount) AS amount FROM recon_data1 WHERE transaction_date <= current_date() - INTERVAL 1 day  and transaction_date between '"+startDate+"' and '"+endDate+"' GROUP BY record_type, processor_name , currency_code,transaction_date,WEEKOFYEAR(transaction_date)";
       log.info("weekWiseQuery: "+weekWiseQuery);
       Dataset<Row> weekWisedata = recon_data1.table("recon_data1").sqlContext().sql(weekWiseQuery).sort("record_type");
       Dataset<Row> weekWisedataPivot=weekWisedata.groupBy("record_type","processor_name").pivot("WEEKOFYEAR(transaction_date)").sum("amount");
       int sz4=weekWisedataPivot.collectAsList().size();
       weekWisedataPivot.show(sz4);*/
       
       DateTimeFormatter dtf = DateTimeFormat.forPattern("yyyy-MM-dd");
       LocalDate curDate = dtf.parseLocalDate("2017-07-04");
       LocalDate startDate = dtf.parseLocalDate("2013-01-01");
       LocalDate endDate = dtf.parseLocalDate("2013-03-31");
       int intreval=30;
       log.info("curDate: "+curDate+" startDate: "+startDate+" endDate: "+endDate);
       //String query="SELECT record_type, processor_name , currency_code,transaction_date,day(transaction_date) AS period, SUM(amount) AS amount FROM recon_data1 WHERE transaction_date <='"+ curDate+"' - INTERVAL 1 day  and transaction_date between '"+startDate+"' and '"+endDate+"' GROUP BY record_type, processor_name , currency_code,transaction_date,period";
       String weekWiseQuery="";
       log.info("weekWiseQuery: "+weekWiseQuery);
       Dataset<Row> weekWisedata = recon_data1.table("recon_data1").sqlContext().sql(weekWiseQuery).sort("currency_code");
       Dataset<Row> weekWisedataPivot=weekWisedata.groupBy("card_type").pivot("WEEKOFYEAR(transaction_date)").sum("amount");
       int sz4=weekWisedataPivot.collectAsList().size();
       weekWisedataPivot.show(sz4);
       
       sContext.close();
       /*Dataset<Row> weekdata = recon_data1.table("recon_data1").sqlContext().sql("SELECT record_type, processor_name , currency_code, amount, year(transaction_date), monthname(transaction_date), Count(*) As Total_Rows FROM recon_data1 GROUP BY record_type, processor_name , currency_code,amount,Year(transaction_date), monthname(transaction_date)").sort("record_type");
       mnthdata.groupBy("record_type","processor_name").pivot("monthname(transaction_date)").sum("amount").show();*/
    }
    
    @PostMapping("/generatePivot")
    @Timed
    public List<LinkedHashMap> generatePivot(@RequestParam Long reportId, @RequestParam Long tenanatId, @RequestBody(required=false) HashMap filtersMap){
    	
    	log.info("Rest Request to retrieve generateReport3 for reportId: "+reportId+" and tenanatId: "+tenanatId);
    	
    	String outputType="";
    	if(filtersMap.containsKey("outputType")){
    		outputType=filtersMap.get("outputType").toString();
    	}
    	
    	Reports reports=reportsRepository.findOne(reportId);
    	Long tenantId=reports.getTenantId();
    	Long repTypeId=reports.getReportTypeId();
    	ReportType repType=reportTypeRepository.findOne(repTypeId);
    	String tableName="";
    	String repTypeName="";
    	if(repType!=null){
    		repTypeName=repType.getType();
    		if(repTypeName.equalsIgnoreCase("ACCOUNT_ANALYSIS_REPORT")){
    			tableName="t_account_analysis";
    		}
    		else if(repTypeName.equalsIgnoreCase("ACCOUNT_BALANCE_REPORT")){
    			tableName="t_balance_type";
    		}
    	}
    	
    	/* Report Defination Columns */
    	List<ReportDefination> repDefList=reportDefinationRepository.findByReportId(reportId);
    	log.info("repDefList: "+repDefList);
        
    	
    	LinkedHashMap fieldRefMap=dataViewsService.getFieldRef(reportId, tableName);
    	log.info("fieldRefMap: "+fieldRefMap);
    	
    	LinkedHashMap<String, String> reversedHashMap = new LinkedHashMap<String, String>();
    	Set<String> keySet=fieldRefMap.keySet();
		for (String key : keySet){
		    reversedHashMap.put(fieldRefMap.get(key).toString(), key);
		}
    	
    	 SparkConf sConf =  new SparkConf().setAppName("Reports").setMaster("yarn-cluster").setMaster("local[*]");
         JavaSparkContext sContext =  new JavaSparkContext(sConf);
         SQLContext sqlCont = new SQLContext(sContext);
         
         long startTime = System.currentTimeMillis();   //gives the start time of a process
         long pivotstartTime = System.currentTimeMillis();   //gives the start time of a pivot
         
  //       Properties props = propertiesUtilService.getPropertiesFromClasspath("File.properties");
     	
   //  	String jdbcDriver = props.getProperty("jdbcDriver");  
   //     String schemaName = props.getProperty("schemaName");  
   //  	String dbUrl = props.getProperty("dbUrl")+schemaName;  
     	
   //  	   String userName = props.getProperty("userName"); 
   //  	   String password = props.getProperty("password"); 
         
  		String dbUrl=env.getProperty("spring.datasource.url");
  		String[] parts=dbUrl.split("[\\s@&?$+-]+");
  		String host = parts[0].split("/")[2].split(":")[0];
  		String schemaName=parts[0].split("/")[3];
  		String userName = env.getProperty("spring.datasource.username");
  		String password = env.getProperty("spring.datasource.password");
  		String jdbcDriver = env.getProperty("spring.datasource.jdbcdriver");
        
         Dataset<Row> reports_data = sqlCont.read().format("jdbc")
                 .option("url", dbUrl)
                 .option("user",userName)
                 .option("password",password)
                 .option("dbtable",tableName).load();
         
        log.info("reports_data.count(): "+reports_data.count());
       
        reports_data.show();
        
        RelationalGroupedDataset grpData = null;
        
        Dataset<Row> data2 = null;
        
        String filter="report_id = "+reportId;
        String finQuery="";
        
        log.info("filtersMap: "+filtersMap);
        
        /* Report Filters */
        
		if(filtersMap!=null){
		
			String conQuery=" where ";
	        String conSubQueryFin="";
			if(filtersMap!=null){
				log.info("filters exists");
	        	if(filtersMap.containsKey("fields")){
	        		List<HashMap> filtersList=(List<HashMap>) filtersMap.get("fields");
	        		log.info("fields exists with sz: "+filtersList.size());
	        		for(int i=0;i<filtersList.size();i++){
	        			HashMap filterMap=filtersList.get(i);
	        			log.info("filterMap: "+filterMap);
	        			String conSubQuery="";
	        			String colName=filterMap.get("fieldName").toString();
	        			log.info("colName: "+colName);
	        			if(filterMap.get("refType").toString().equalsIgnoreCase("FIN_FUNCTION")){
	        				Long refColId=Long.parseLong(filterMap.get("refColId").toString());
	        				LookUpCode finFunCode=lookUpCodeRepository.findOne(refColId);
	        				log.info("finFunCode: "+finFunCode);
	        				colName=finFunCode.getLookUpCode().toLowerCase();
	        			}
	        			else {
	        				colName=fieldRefMap.get(colName).toString();
	        			}
	        			String selType=filterMap.get("fieldType").toString();
	        			log.info("selType: "+selType);
	        			if(selType.equalsIgnoreCase("MULTI_SELECT_LOV") || selType.equalsIgnoreCase("SINGLE_SELECT_LOV")  || selType.equalsIgnoreCase("SINGLE_SELECTION") || selType.equalsIgnoreCase("TEXT")){
	        				String selValData="";
	        				String selValVar=filterMap.get("selectedValue").toString();
	        				log.info("selValVar: "+selValVar);
	        				selValVar=selValVar.replace("[", "");
	        				selValVar=selValVar.replace("]", "");
	        				List<String> selValList = new ArrayList<String>(Arrays.asList(selValVar.split(",")));
	        				for(int k=0;k<selValList.size();k++){
	        					String selVal=selValList.get(k);
	        					selVal=selVal.trim();
	        					selValData=selValData+"'"+selVal+"'";
	        					if(k>=0 && k<selValList.size()-1){
	        						selValData=selValData+",";
	        					}
	        				}
	        				if(selValData!=null && !(selValData.isEmpty())){
	        					if(selValData.endsWith(",")){
	        						int indx=selValData.lastIndexOf(",");
	        						selValData=selValData.substring(0,indx);
	        						log.info("selValData after removing comma: "+selValData);
	        					}
	        					conSubQuery=conSubQuery+colName+" in ("+selValData+") ";
	        					log.info("conSubQuery: "+conSubQuery);
	        				}
	        			}
	        			else if(selType.equalsIgnoreCase("AUTO_COMPLETE")){
	        				String selVal=filterMap.get("selectedValue").toString();
	        				if(selVal!=null && !(selVal.isEmpty())){
		        				conSubQuery=conSubQuery+colName+" in ('"+selVal+"') ";
		        				}
	        			}
	        			else if(selType.equalsIgnoreCase("BOOLEAN_SELECTION")){
	        				String selVal=filterMap.get("selectedValue").toString();
	        				if(selVal!=null && !(selVal.isEmpty())){
	        					if(conSubQuery.equalsIgnoreCase(" where ")){
	        	        			
	        	        		}
	        					else conSubQuery=" and ";
	        				conSubQuery=conSubQuery+colName+" is "+selVal;
	        				}
	        			}
	        			else if(selType.equalsIgnoreCase("AMOUNT_RANGE")){
	        				String map=filterMap.get("selectedValue").toString();
	        				if(map!=null && !(map.isEmpty())){
	        				log.info("map: "+map);
	        				//String map="selectedValue={fromValue=1036873.4, toValue=1377186}";
	        				map=map.replace("{fromValue=", ""); //1036873.4, toValue=1377186}"
	        				map=map.replace("toValue=", ""); //1036873.4,1377186}
	        				log.info("map aftr replace: "+map);
	        				String arr[]=map.split(",");
	        				log.info(" arr[0]: "+arr[0]);
	        				log.info("arr[1]: "+arr[1]);
	        				arr[1]=arr[1].replace("}", "");
	        				System.out.println("arr[1]: "+arr[1]+" arr[0]: "+arr[0]);
	        				String fromValue=arr[0].trim();
	        				String toValue=arr[1].trim();
	        				conSubQuery=conSubQuery+colName+" between "+fromValue+" and "+toValue;	
	        				}
	        			}
	        		log.info("conSubQuery: "+conSubQuery);
	        		if(conSubQuery.length()>1){
	        			conSubQuery=conSubQuery+" and ";
	        		}
	        		conSubQueryFin=conSubQuery;
	        		if(conSubQuery.equalsIgnoreCase(" where ")){
	        			
	        		}
	        		else{
	        			finQuery=finQuery+conSubQuery;
	        		}
	        			log.info("finQuery after conQuery: "+finQuery);
	        		}
	        		
	        	}
	        }
			else{
				log.info("filters doesnt exist");
			}
	        log.info("finQuery after filters: "+finQuery);
	        log.info("conSubQueryFin: "+conSubQueryFin);
		}
		else{
			
		}
		if(finQuery.endsWith("and ")){
        	int indx=finQuery.lastIndexOf("and");
        	finQuery=finQuery.substring(0,indx);
        }
		if(finQuery!=null && !(finQuery.isEmpty())){
			
			finQuery=finQuery+" and "+filter;
		}
		else {

			log.info("filter: "+filter);
			finQuery=filter;
			log.info("finQuery:" +finQuery);
		}
        
        log.info("finQuery: "+finQuery);
        Dataset<Row> dt = reports_data.filter(finQuery);
        reports_data=dt;
        
        int sz=0;
        String pivotCol="";
        String refPivotCol="";
        List grpColList=new ArrayList<>();
        
        /* Pivot Rows */
        if(filtersMap.containsKey("groupingCols")){
        	grpColList=(List) filtersMap.get("groupingCols");
        log.info("grpColList: "+grpColList);
        sz=grpColList.size();
        log.info("sz: "+grpColList);
        org.apache.spark.sql.Column[] col = new org.apache.spark.sql.Column[sz];
        int h=0;
        String refName="";
        for(h=0;h<sz;h++){
        	HashMap map=(HashMap) grpColList.get(h);
        	String dpName=map.get("userDisplayColName").toString();
        	
        	if(map.get("columnType").toString().equalsIgnoreCase("FIN_FUNCTION")){
        		refName=map.get("columnDisplayName").toString();
        		
        	}
        	else refName=fieldRefMap.get(dpName).toString();
        	
            col[h]= new Column(refName);
            log.info("col["+h+"]: "+col[h]);
        }
        
        log.info("col: "+col);
        
        grpData=reports_data.groupBy(col);
        
        log.info("grouped data count: "+grpData.count());
        
        
        /* Pivot Columns */
        if(filtersMap.containsKey("columnCols")){
        	
        	List pivotCols=(List) filtersMap.get("columnCols");
        	log.info("pivotCols: "+pivotCols);
        	HashMap pivotColObj=(HashMap) pivotCols.get(0);
        	pivotCol=pivotColObj.get("userDisplayColName").toString();
        	refPivotCol=fieldRefMap.get(pivotCol).toString();
        	grpData=grpData.pivot(refPivotCol);
        }
        
        log.info("pivoted data count: "+grpData.count());
        }
        
        String amtCol="";
        /* Aggregation */
        if(filtersMap.containsKey("valueCols")){
        	
        	List amtColList=(List) filtersMap.get("valueCols");
        	HashMap amtColMap=(HashMap) amtColList.get(0);
        	log.info("amtColMap: "+amtColMap);
        	amtCol=amtColMap.get("userDisplayColName").toString();
        	String refAmtCol="";
        	if(amtColMap.get("columnType").toString().equalsIgnoreCase("FIN_FUNCTION")){
        		refAmtCol=amtColMap.get("columnDisplayName").toString();
        		
        	}
        	else refAmtCol=fieldRefMap.get(amtCol).toString();
        	
            
            data2=grpData.sum(refAmtCol);
            log.info("data2 after addng sum cnt: "+data2.count());
            
            data2.show();
        	
        }
    
        List<Row> data=data2.collectAsList();
        String[] columnsList=data2.columns();
        log.info("data.size(): "+data.size()+" columnsList.length: "+columnsList.length);
        
        /* Final Result set */
        List<LinkedHashMap> maps=new ArrayList<LinkedHashMap>();
        for(int j=0;j<data.size();j++){
            
            for(int s=0;s<(columnsList.length);s++){
            	
            	if(s+sz==columnsList.length || j+sz==columnsList.length){
            		break;
            	}
            	LinkedHashMap map=new LinkedHashMap();
             	for(int z=0;z<sz;z++){
             		HashMap grpColMap=(HashMap) grpColList.get(z);
             		log.info("grpColMap: "+grpColMap);
                	map.put(grpColMap.get("userDisplayColName"),data.get(j).get(z));		
                }
             	
             	if(pivotCol!=null && !(pivotCol.isEmpty())){
             	map.put(pivotCol,columnsList[s+sz]);
             	}
             	
             	if(data.get(j).get(s+sz)!=null)
             	map.put(amtCol,data.get(j).get(s+sz));
                maps.add(map);
            }
            
        }
        sContext.close();
		return maps;
    }
    
    @PostMapping("/generateChart")
    @Timed
    public LinkedHashMap generateChart(@RequestParam Long reportId, @RequestParam Long tenanatId, @RequestBody(required=false) HashMap filtersMap){
    	
    	log.info("Rest Request to retrieve generateReport3 for reportId: "+reportId+" and tenanatId: "+tenanatId);
    	
    	String outputType="";
    	if(filtersMap.containsKey("outputType")){
    		outputType=filtersMap.get("outputType").toString();
    	}
    	
    	Reports reports=reportsRepository.findOne(reportId);
    	Long tenantId=reports.getTenantId();
    	Long repTypeId=reports.getReportTypeId();
    	ReportType repType=reportTypeRepository.findOne(repTypeId);
    	String tableName="";
    	String repTypeName="";
    	if(repType!=null){
    		repTypeName=repType.getType();
    		if(repTypeName.equalsIgnoreCase("ACCOUNT_ANALYSIS_REPORT")){
    			tableName="t_account_analysis";
    		}
    		else if(repTypeName.equalsIgnoreCase("ACCOUNT_BALANCE_REPORT")){
    			tableName="t_balance_type";
    		}
    	}
    	
    	/* Report Defination Columns */
    	List<ReportDefination> repDefList=reportDefinationRepository.findByReportId(reportId);
    	log.info("repDefList: "+repDefList);
        
    	
    	LinkedHashMap fieldRefMap=dataViewsService.getFieldRef(reportId, tableName);
    	log.info("fieldRefMap: "+fieldRefMap);
    	
    	LinkedHashMap<String, String> reversedHashMap = new LinkedHashMap<String, String>();
    	Set<String> keySet=fieldRefMap.keySet();
		for (String key : keySet){
		    reversedHashMap.put(fieldRefMap.get(key).toString(), key);
		}
    	
    	 SparkConf sConf =  new SparkConf().setAppName("Reports").setMaster("yarn-cluster").setMaster("local[*]");
         JavaSparkContext sContext =  new JavaSparkContext(sConf);
         SQLContext sqlCont = new SQLContext(sContext);
         
         long startTime = System.currentTimeMillis();   //gives the start time of a process
         long pivotstartTime = System.currentTimeMillis();   //gives the start time of a pivot
         
  		String dbUrl=env.getProperty("spring.datasource.url");
  		String[] parts=dbUrl.split("[\\s@&?$+-]+");
  		String host = parts[0].split("/")[2].split(":")[0];
  		String schemaName=parts[0].split("/")[3];
  		String userName = env.getProperty("spring.datasource.username");
  		String password = env.getProperty("spring.datasource.password");
  		String jdbcDriver = env.getProperty("spring.datasource.jdbcdriver");
  		
  		 DataFrameReader dfr = sqlCont.read().format("jdbc")
                 .option("url", dbUrl)
                 .option("user",userName)
                 .option("password",password)
                 .option("dbtable",tableName);
    	  
    	Dataset<Row> reports_data = dfr.load();
       
    	reports_data.registerTempTable("dfr");
         
        log.info("reports_data.count(): "+reports_data.count());
       
        reports_data.show();
        
        String filter="report_id = "+reportId;
        String finQuery="";
        
        log.info("filtersMap: "+filtersMap);
        
        /* Report Filters */
        
		if(filtersMap!=null){
		
			String conQuery=" where ";
	        String conSubQueryFin="";
			if(filtersMap!=null){
				log.info("filters exists");
	        	if(filtersMap.containsKey("fields")){
	        		List<HashMap> filtersList=(List<HashMap>) filtersMap.get("fields");
	        		log.info("fields exists with sz: "+filtersList.size());
	        		for(int i=0;i<filtersList.size();i++){
	        			HashMap filterMap=filtersList.get(i);
	        			log.info("filterMap: "+filterMap);
	        			String conSubQuery="";
	        			String colName=filterMap.get("fieldName").toString();
	        			log.info("colName: "+colName);
	        			if(filterMap.get("refType").toString().equalsIgnoreCase("FIN_FUNCTION")){
	        				Long refColId=Long.parseLong(filterMap.get("refColId").toString());
	        				LookUpCode finFunCode=lookUpCodeRepository.findOne(refColId);
	        				log.info("finFunCode: "+finFunCode);
	        				colName=finFunCode.getLookUpCode().toLowerCase();
	        			}
	        			else {
	        				colName=fieldRefMap.get(colName).toString();
	        			}
	        			String selType=filterMap.get("fieldType").toString();
	        			log.info("selType: "+selType);
	        			if(selType.equalsIgnoreCase("MULTI_SELECT_LOV") || selType.equalsIgnoreCase("SINGLE_SELECT_LOV")  || selType.equalsIgnoreCase("SINGLE_SELECTION") || selType.equalsIgnoreCase("TEXT")){
	        				String selValData="";
	        				String selValVar=filterMap.get("selectedValue").toString();
	        				log.info("selValVar: "+selValVar);
	        				selValVar=selValVar.replace("[", "");
	        				selValVar=selValVar.replace("]", "");
	        				List<String> selValList = new ArrayList<String>(Arrays.asList(selValVar.split(",")));
	        				for(int k=0;k<selValList.size();k++){
	        					String selVal=selValList.get(k);
	        					selVal=selVal.trim();
	        					selValData=selValData+"'"+selVal+"'";
	        					if(k>=0 && k<selValList.size()-1){
	        						selValData=selValData+",";
	        					}
	        				}
	        				if(selValData!=null && !(selValData.isEmpty())){
	        					if(selValData.endsWith(",")){
	        						int indx=selValData.lastIndexOf(",");
	        						selValData=selValData.substring(0,indx);
	        						log.info("selValData after removing comma: "+selValData);
	        					}
	        					conSubQuery=conSubQuery+colName+" in ("+selValData+") ";
	        					log.info("conSubQuery: "+conSubQuery);
	        				}
	        			}
	        			else if(selType.equalsIgnoreCase("AUTO_COMPLETE")){
	        				String selVal=filterMap.get("selectedValue").toString();
	        				if(selVal!=null && !(selVal.isEmpty())){
		        				conSubQuery=conSubQuery+colName+" in ('"+selVal+"') ";
		        				}
	        			}
	        			else if(selType.equalsIgnoreCase("BOOLEAN_SELECTION")){
	        				String selVal=filterMap.get("selectedValue").toString();
	        				if(selVal!=null && !(selVal.isEmpty())){
	        					if(conSubQuery.equalsIgnoreCase(" where ")){
	        	        			
	        	        		}
	        					else conSubQuery=" and ";
	        				conSubQuery=conSubQuery+colName+" is "+selVal;
	        				}
	        			}
	        			else if(selType.equalsIgnoreCase("AMOUNT_RANGE")){
	        				String map=filterMap.get("selectedValue").toString();
	        				if(map!=null && !(map.isEmpty())){
	        				log.info("map: "+map);
	        				//String map="selectedValue={fromValue=1036873.4, toValue=1377186}";
	        				map=map.replace("{fromValue=", ""); //1036873.4, toValue=1377186}"
	        				map=map.replace("toValue=", ""); //1036873.4,1377186}
	        				log.info("map aftr replace: "+map);
	        				String arr[]=map.split(",");
	        				log.info(" arr[0]: "+arr[0]);
	        				log.info("arr[1]: "+arr[1]);
	        				arr[1]=arr[1].replace("}", "");
	        				System.out.println("arr[1]: "+arr[1]+" arr[0]: "+arr[0]);
	        				String fromValue=arr[0].trim();
	        				String toValue=arr[1].trim();
	        				conSubQuery=conSubQuery+colName+" between "+fromValue+" and "+toValue;	
	        				}
	        			}
	        		log.info("conSubQuery: "+conSubQuery);
	        		if(conSubQuery.length()>1){
	        			conSubQuery=conSubQuery+" and ";
	        		}
	        		conSubQueryFin=conSubQuery;
	        		if(conSubQuery.equalsIgnoreCase(" where ")){
	        			
	        		}
	        		else{
	        			finQuery=finQuery+conSubQuery;
	        		}
	        			log.info("finQuery after conQuery: "+finQuery);
	        		}
	        		
	        	}
	        }
			else{
				log.info("filters doesnt exist");
			}
	        log.info("finQuery after filters: "+finQuery);
	        log.info("conSubQueryFin: "+conSubQueryFin);
		}
		else{
			log.info("filters does't exist");
		}
		if(finQuery.endsWith("and ")){
        	int indx=finQuery.lastIndexOf("and");
        	finQuery=finQuery.substring(0,indx);
        }
		if(finQuery!=null && !(finQuery.isEmpty())){
			
			finQuery=finQuery+" and "+filter;
		}
		else {

			log.info("filter: "+filter);
			finQuery=filter;
			log.info("finQuery:" +finQuery);
		}
        log.info("after applying filters");
        reports_data.show();
        
        int grpSz=0;
        int colSz=0;
        String pivotCol="";
        String refPivotCol="";
        List grpColList=new ArrayList<>();
        List valueColsList=new ArrayList<>();
        
        HashMap grpmap1=new HashMap();
        HashMap colmap1=new HashMap();
        String grprefName="";
        String colrefName="";
        /* Pivot Rows */
        if(filtersMap.containsKey("groupingCols")){
        	grpColList=(List) filtersMap.get("groupingCols");
        log.info("grpColList: "+grpColList);
        grpSz=grpColList.size();
        log.info("sz: "+grpColList);
        }
        
        if(filtersMap.containsKey("valueCols")){
        	valueColsList=(List) filtersMap.get("valueCols");
        log.info("valueColsList: "+valueColsList);
       
        log.info("valueColsList sz: "+valueColsList.size());
        colSz=valueColsList.size();
        }
        log.info("valueColsList: "+valueColsList);
        
        HashMap refMap=new HashMap();
        List<String> amtColList=new ArrayList<>();
        int h1=0;
        for(h1=0;h1<colSz;h1++){
        	colmap1=(HashMap) valueColsList.get(h1);
        	String dpName=colmap1.get("userDisplayColName").toString();
        	

        	if(colmap1.get("columnType").toString().equalsIgnoreCase("FIN_FUNCTION")){
        		colrefName=colmap1.get("columnDisplayName").toString();
        	}
        	else colrefName=fieldRefMap.get(dpName).toString();
        	refMap.put(colrefName, dpName);
        	log.info("colrefName: "+colrefName);
        	amtColList.add(colrefName);
        }
        log.info("amtColList: "+amtColList);
        org.apache.spark.sql.Column[] col = new org.apache.spark.sql.Column[grpSz];
        
        int h=0;
        for(h=0;h<grpSz;h++){
        	grpmap1=(HashMap) grpColList.get(0);
        	String dpName=grpmap1.get("userDisplayColName").toString();
        	
        	if(grpmap1.get("columnType").toString().equalsIgnoreCase("FIN_FUNCTION")){
        		grprefName=grpmap1.get("columnDisplayName").toString();
        		
        		
        	}
        	else grprefName=fieldRefMap.get(dpName).toString();
        	 col[h]= new Column(grprefName);
        	 log.info("col["+h+"]: "+col[h]);
        }
        
        log.info("col.length: "+col.length);
        log.info("grp col: "+col);
        log.info("refMap: "+refMap);
        
        String Query="select field_02, sum(OPENING_BALANCE), sum(ADDITIONS_AMT) from dfr "+" where "+finQuery+" group by field_02 ";
        Dataset<Row> weekWisedata = dfr.table("dfr").sqlContext().sql(Query).sort("field_02");
        int sz4=weekWisedata.collectAsList().size();
        weekWisedata.show(sz4);
        List lablsList=weekWisedata.select(col[0]).distinct().collectAsList();
        log.info("lablsList: "+lablsList);
        List<String> finLbleList=new ArrayList<String>();
        for(int k=0;k<lablsList.size();k++){
        	String lable=lablsList.get(k).toString();
        	lable=lable.replace('[', ' ');
        	lable=lable.replace(']', ' ');
        	lable=lable.trim();
        	finLbleList.add(lable);
        }
        log.info("finLbleList: "+finLbleList);
        List<Row> data=weekWisedata.collectAsList();
        String[] columnsList=weekWisedata.columns();
        
        LinkedHashMap finMap=new LinkedHashMap();
        finMap.put("labels", finLbleList);
        
        log.info("data.size(): "+data.size()+" columnsList.length: "+columnsList.length);
        int sz=data.size();
        List<LinkedHashMap> mapList=new ArrayList<LinkedHashMap>();
        /* Final Result set */
        log.info("columnsList: "+columnsList);
        
        List<LinkedHashMap> ListOfMap = new ArrayList<>();
        for(int m=1;m<columnsList.length;m++)
        {
        	 List dataList=new ArrayList<>();
        	LinkedHashMap map=new LinkedHashMap();
        	log.info("m value: "+m);
        	for(int n=0;n<data.size();n++)
        	{
        		log.info("N avlue: "+n);
        		log.info(" m, n avlues: ("+n+","+m+")");
        		
        		
        		log.info("columnsList[m]: "+columnsList[m]);
        		
        		columnsList[m]=columnsList[m].replace("sum(", "");
        		columnsList[m]=columnsList[m].replace(")", "");
        		if(map!=null && !(map.isEmpty())&& !(map.get("label").toString().isEmpty()) && map.get("label").toString().equalsIgnoreCase(refMap.get(columnsList[m]).toString()))
    			{
    				
        			log.info("In If when n: "+n+" and m: "+m);
    				dataList=(List) map.get("data");
    				dataList.add(data.get(n).get(m));
    			}
    			else{
    				log.info("In Else when n: "+n+" and m: "+m+" and dataList: "+dataList);
    				dataList.add(data.get(n).get(m));
    			}
        	}
        	columnsList[m]=columnsList[m].replace("sum(", "");
    		columnsList[m]=columnsList[m].replace(")", "");
        	map.put("label",refMap.get(columnsList[m]));
        	map.put("data", dataList);
        	map.put("backgroundColor", "");
            map.put("fill",false);
    		if(map!=null)
    			ListOfMap.add(map);
    		log.info("Map Values: "+map);
        }
        log.info("List of maps size: "+ListOfMap.size());
        log.info("List of maps  :"+ListOfMap);
        
        finMap.put("datasets", ListOfMap);
        sContext.close();
        return finMap;
    }
}