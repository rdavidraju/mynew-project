package com.nspl.app.web.rest;

import com.codahale.metrics.annotation.Timed;
import com.nspl.app.domain.DataViewsColumns;
import com.nspl.app.domain.FileTemplateLines;
import com.nspl.app.domain.JobActions;
import com.nspl.app.domain.LookUpCode;
import com.nspl.app.domain.Notifications;
import com.nspl.app.domain.Project;
import com.nspl.app.domain.ReportDefination;
import com.nspl.app.domain.ReportParameters;
import com.nspl.app.domain.ReportRequests;
import com.nspl.app.domain.ReportType;
import com.nspl.app.domain.Reports;
import com.nspl.app.domain.SchedulerDetails;
import com.nspl.app.domain.Segments;
import com.nspl.app.domain.TenantConfig;
import com.nspl.app.repository.BucketDetailsRepository;
import com.nspl.app.repository.BucketListRepository;
import com.nspl.app.repository.DataViewsColumnsRepository;
import com.nspl.app.repository.DataViewsRepository;
import com.nspl.app.repository.FileTemplateLinesRepository;
import com.nspl.app.repository.JobActionsRepository;
import com.nspl.app.repository.LookUpCodeRepository;
import com.nspl.app.repository.NotificationsRepository;
import com.nspl.app.repository.ProjectRepository;
import com.nspl.app.repository.ReportDefinationRepository;
import com.nspl.app.repository.ReportParametersRepository;
import com.nspl.app.repository.ReportRequestsRepository;
import com.nspl.app.repository.ReportsRepository;
import com.nspl.app.repository.SchedulerDetailsRepository;
import com.nspl.app.repository.TenantConfigRepository;
import com.nspl.app.repository.search.ProjectSearchRepository;
import com.nspl.app.repository.SegmentsRepository;
import com.nspl.app.service.DataViewsService;
//import com.nspl.app.service.LivySparkService;
import com.nspl.app.service.OozieService;
import com.nspl.app.service.PropertiesUtilService;
import com.nspl.app.service.ReportsService;
import com.nspl.app.service.UserJdbcService;
import com.nspl.app.web.rest.util.HeaderUtil;

/*import com.nspl.livy.InteractiveJobParameters;
import com.nspl.livy.LivyException;
import com.nspl.livy.LivyInteractiveClient;
import com.nspl.livy.SessionEventListener;
import com.nspl.livy.SessionKind;
import com.nspl.livy.StatementResult;
import com.nspl.livy.StatementResultListener;*/
import io.github.jhipster.web.util.ResponseUtil;

import org.apache.commons.collections.MapUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.oozie.client.OozieClientException;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.AnalysisException;
import org.apache.spark.sql.Column;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.RelationalGroupedDataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;
import org.joda.time.DateTime;
import org.json.JSONException;
import org.json.simple.JSONObject;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.core.env.Environment;
import org.springframework.http.ResponseEntity;
import org.springframework.scheduling.annotation.Async;
import org.springframework.web.bind.annotation.*;

import java.io.IOException;
import java.math.BigDecimal;
import java.net.MalformedURLException;
import java.net.URI;
import java.net.URISyntaxException;
import java.security.InvalidAlgorithmParameterException;
import java.security.InvalidKeyException;
import java.security.NoSuchAlgorithmException;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.ResultSet;
import java.sql.ResultSetMetaData;
import java.sql.SQLException;
import java.sql.Statement;
import java.text.ParseException;
import java.time.ZonedDateTime;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Date;
import java.util.HashMap;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.stream.Collectors;
import java.util.stream.StreamSupport;

import scala.Tuple2;
import scala.collection.mutable.StringBuilder;

import javax.inject.Inject;
import javax.persistence.EntityManager;
import javax.persistence.PersistenceContext;
import javax.persistence.Query;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;

import static org.elasticsearch.index.query.QueryBuilders.*;

/**
 * REST controller for managing Project.
 */
@RestController
@RequestMapping("/api")
public class ProjectResource {

    private final Logger log = LoggerFactory.getLogger(ProjectResource.class);

    private static final String ENTITY_NAME = "project";
        
    private final ProjectRepository projectRepository;

    private final ProjectSearchRepository projectSearchRepository;
    
    
    @Inject 
    ReportsRepository reportsRepository;
    
    @Inject
    DataViewsRepository dataViewsRepository;
    
    @Inject
    ReportParametersRepository reportParametersRepository;
    
    @Inject
    LookUpCodeRepository lookUpCodeRepository;
    
    @Inject
    DataViewsService dataViewsService;
    
    @Inject
    ReportDefinationRepository reportDefinationRepository;
    
    @Inject
    private Environment env;
    
    @Inject
    ReportsService reportsService;
    
    @Inject
    BucketListRepository bucketListRepository;
    
    @Inject
    BucketDetailsRepository bucketDetailsRepository;
    
    @Inject
    DataViewsColumnsRepository dataViewsColumnsRepository;
    
    @Inject
    FileTemplateLinesRepository fileTemplateLinesRepository;
    
    @Inject
    PropertiesUtilService propertiesUtilService;
    
    @Inject
    OozieService oozieService;
    
    @Inject
    JobActionsRepository jobActionsRepository;
    
    @Inject
    ReportRequestsRepository reportRequestsRepository;
    
    @Inject
    NotificationsRepository notificationsRepository;
    
    @Inject
    SchedulerDetailsRepository schedulerDetailsRepository;
    
    @Inject
    SegmentsRepository segmentsRepository;
    
    @Inject
    UserJdbcService userJdbcService;
    
    @Inject
    TenantConfigRepository tenantConfigRepository;
    
   
    @PersistenceContext(unitName="default")
	private EntityManager em;
    
/*    private LivyInteractiveClient client = null;
	private int session_status = com.nspl.livy.Session.STARTING;
	private String resultResponse = null;*/
	
	/*@Inject
	LivySparkService livySparkService;*/


    public ProjectResource(ProjectRepository projectRepository, ProjectSearchRepository projectSearchRepository) {
        this.projectRepository = projectRepository;
        this.projectSearchRepository = projectSearchRepository;
    }

    /**
     * POST  /projects : Create a new project.
     *
     * @param project the project to create
     * @return the ResponseEntity with status 201 (Created) and with body the new project, or with status 400 (Bad Request) if the project has already an ID
     * @throws URISyntaxException if the Location URI syntax is incorrect
     */
    @PostMapping("/projects")
    @Timed
    public ResponseEntity<Project> createProject(@RequestBody Project project) throws URISyntaxException {
        log.debug("REST request to save Project : {}", project);
        if (project.getId() != null) {
            return ResponseEntity.badRequest().headers(HeaderUtil.createFailureAlert(ENTITY_NAME, "idexists", "A new project cannot already have an ID")).body(null);
        }
        Project result = projectRepository.save(project);
        projectSearchRepository.save(result);
        return ResponseEntity.created(new URI("/api/projects/" + result.getId()))
            .headers(HeaderUtil.createEntityCreationAlert(ENTITY_NAME, result.getId().toString()))
            .body(result);
    }

    /**
     * PUT  /projects : Updates an existing project.
     *
     * @param project the project to update
     * @return the ResponseEntity with status 200 (OK) and with body the updated project,
     * or with status 400 (Bad Request) if the project is not valid,
     * or with status 500 (Internal Server Error) if the project couldnt be updated
     * @throws URISyntaxException if the Location URI syntax is incorrect
     */
    @PutMapping("/projects")
    @Timed
    public ResponseEntity<Project> updateProject(@RequestBody Project project) throws URISyntaxException {
        log.debug("REST request to update Project : {}", project);
        if (project.getId() == null) {
            return createProject(project);
        }
        Project result = projectRepository.save(project);
        projectSearchRepository.save(result);
        return ResponseEntity.ok()
            .headers(HeaderUtil.createEntityUpdateAlert(ENTITY_NAME, project.getId().toString()))
            .body(result);
    }

    /**
     * GET  /projects : get all the projects.
     *
     * @return the ResponseEntity with status 200 (OK) and the list of projects in body
     */
    @GetMapping("/projects")
    @Timed
    public List<Project> getAllProjects() {
        log.debug("REST request to get all Projects");
        List<Project> projects = projectRepository.findAll();
        return projects;
    }

    /**
     * GET  /projects/:id : get the "id" project.
     *
     * @param id the id of the project to retrieve
     * @return the ResponseEntity with status 200 (OK) and with body the project, or with status 404 (Not Found)
     */
    @GetMapping("/projects/{id}")
    @Timed
    public ResponseEntity<Project> getProject(@PathVariable Long id) {
        log.debug("REST request to get Project : {}", id);
        Project project = projectRepository.findOne(id);
        return ResponseUtil.wrapOrNotFound(Optional.ofNullable(project));
    }

    /**
     * DELETE  /projects/:id : delete the "id" project.
     *
     * @param id the id of the project to delete
     * @return the ResponseEntity with status 200 (OK)
     */
    @DeleteMapping("/projects/{id}")
    @Timed
    public ResponseEntity<Void> deleteProject(@PathVariable Long id) {
        log.debug("REST request to delete Project : {}", id);
        projectRepository.delete(id);
        projectSearchRepository.delete(id);
        return ResponseEntity.ok().headers(HeaderUtil.createEntityDeletionAlert(ENTITY_NAME, id.toString())).build();
    }

    /**
     * SEARCH  /_search/projects?query=:query : search for the project corresponding
     * to the query.
     *
     * @param query the query of the project search 
     * @return the result of the search
     */
    @GetMapping("/_search/projects")
    @Timed
    public List<Project> searchProjects(@RequestParam String query) {
        log.debug("REST request to search Projects for query {}", query);
        return StreamSupport
            .stream(projectSearchRepository.search(queryStringQuery(query)).spliterator(), false)
            .collect(Collectors.toList());
    }

    /**
     * POC for Accounting Work Queue Pivot
     * @param tenanatId
     * @param reportId
     * @param filtersMap
     * @return
     * @throws AnalysisException
     * @throws IOException
     * @throws ParseException
     */
    @PostMapping("/AWQPivotingData")
    @Timed
    public List<LinkedHashMap> AWQ_poc(@RequestParam Long tenanatId, @RequestBody(required=false) HashMap filtersMap, @RequestParam Long dataViewId, @RequestParam Long ruleGrpId) throws AnalysisException, IOException, ParseException{
    	
    	SparkSession spark = reportsService.getSparkSession();
    	JavaSparkContext sContext = new JavaSparkContext(spark.sparkContext());
    	SQLContext sqlCont = new SQLContext(sContext);
    	
    	
    	Dataset<Row> reports_data=reportsService.pocAWQ(tenanatId, filtersMap,spark, dataViewId,ruleGrpId);
    	
    	log.info("Previewing reports_data b4 Pivoting");
    	reports_data.show();
    	
    	int sz=0;
        String pivotCol="";
        String refPivotCol="";
        List grpColList=new ArrayList<>();
        
        RelationalGroupedDataset grpData = null;
        Dataset<Row> data2 = null;
        
        String dbUrl = env.getProperty("spring.datasource.url");
		String[] parts = dbUrl.split("[\\s@&?$+-]+");
		String host = parts[0].split("/")[2].split(":")[0];
		String schemaName = parts[0].split("/")[3];
		String userName = env.getProperty("spring.datasource.username");
		String password = env.getProperty("spring.datasource.password");
		String jdbcDriver = env.getProperty("spring.datasource.jdbcdriver");
        
        /*String dbUrl = "jdbc:mysql://192.168.0.44:3306/agree_application_2712";
		//String[] parts = dbUrl.split("[\\s@&?$+-]+");
		String host = "192.168.0.44";
		String schemaName = "agree_application_2712";
		String userName = "recon_dev";
		String password = "Welcome321$";
		//String jdbcDriver = "";
*/		
        Dataset<Row> datViewData = sqlCont.read().format("jdbc")
				.option("url", dbUrl).option("user", userName)
				.option("password", password).option("dbtable", "t_data_views")
				.load().where("id=" + dataViewId).select("data_view_name");

		Row dv = datViewData.collectAsList().get(0);

		String dvatViewName = dv.getString(0);
		dvatViewName = dvatViewName.toLowerCase();
		
       /*  Pivot Rows */
        if(filtersMap.containsKey("groupingCols")){
        	grpColList=(List) filtersMap.get("groupingCols");
        log.info("grpColList: "+grpColList);
        sz=grpColList.size();
        log.info("sz: "+grpColList);
        org.apache.spark.sql.Column[] col = new org.apache.spark.sql.Column[sz];
        int h=0;
        String refName="";
        for(h=0;h<sz;h++){
        	HashMap map=(HashMap) grpColList.get(h);
        	log.info("map: "+map);
        	String refTypeId=map.get("refType").toString();
        	if(refTypeId.equalsIgnoreCase("FIN_FUNCTION")){
        		refName=map.get("itemName").toString();
        		
        	}
        	else {
        		refName=map.get("itemName").toString();
        	}
        	
            col[h]= new Column(refName);
            log.info("col["+h+"]: "+col[h]);
        }
        
        log.info("col: "+col.toString());
        
        grpData=reports_data.groupBy(col);

        log.info("grouped data count: "+grpData.count());
        
        
       /*  Pivot Columns */
        if(filtersMap.containsKey("columnCols")){
        	
        	List pivotCols=(List) filtersMap.get("columnCols");
        	log.info("pivotCols: "+pivotCols);
        	HashMap pivotColObj=(HashMap) pivotCols.get(0);
        	pivotCol=pivotColObj.get("itemName").toString();
        	//refPivotCol=fieldRefMap.get(pivotCol).toString();
        	refPivotCol=pivotCol;
        	grpData=grpData.pivot(refPivotCol);
        }
        
        log.info("pivoted data count: "+grpData.count());
        }
        
        String amtCol="";
       /*  Aggregation */
        if(filtersMap.containsKey("valueCols")){
        	List amtColList=(List) filtersMap.get("valueCols");
        	HashMap amtColMap=(HashMap) amtColList.get(0);
        	log.info("amtColMap: "+amtColMap);
        	amtCol=amtColMap.get("itemName").toString();
        	String refAmtCol="";
        	String amtColrefTypeId=amtColMap.get("refType").toString();
        	refAmtCol=amtColMap.get("itemName").toString();
            data2=grpData.agg(
                functions.sum(refAmtCol).as("sum"),
                functions.count("*").as("count")
            );    
            log.info("data2 after addng sum cnt: "+data2.count());
            data2.show();
        }
    
        List<Row> data=data2.collectAsList();
        String[] columnsList=data2.columns();
        log.info("data.size(): "+data.size()+" columnsList.length: "+columnsList.length);
        
        /* Final Result set */
        List<LinkedHashMap> maps=new ArrayList<LinkedHashMap>();
        log.info(">> "+ data.get(0).get(0));
        
        for(int j=0;j<data.size();j++){
        	LinkedHashMap mapCount = new LinkedHashMap();
        	LinkedHashMap mapAmount = new LinkedHashMap();
        	String amount = "";
        	String count = "";
        	for(int s=0; s<columnsList.length; s++)
        	{
        		if(!("sum".equalsIgnoreCase(columnsList[s])) && !("count".equalsIgnoreCase(columnsList[s])))
        		{
        			mapCount.put(columnsList[s], data.get(j).get(s));
        			mapAmount.put(columnsList[s], data.get(j).get(s));
        		}
        		else if("sum".equalsIgnoreCase(columnsList[s]))
        		{
        			amount = amount + data.get(j).get(s).toString();
        		}
        		else if("count".equalsIgnoreCase(columnsList[s]))
        		{
        			count = count + data.get(j).get(s).toString();
        		}
        	}
        	mapCount.put("type", "count");
        	mapCount.put("value", Integer.parseInt(count));
        	
        	mapAmount.put("type", "amount");
        	mapAmount.put("value", Double.parseDouble(amount));
        	
        	maps.add(mapCount);
        	maps.add(mapAmount);
/*            for(int s=0;s<(columnsList.length);s++){
            	
            	if(s+sz==columnsList.length || j+sz==columnsList.length){
            		break;
            	}
            	LinkedHashMap map = new LinkedHashMap();
            	LinkedHashMap map2 = new LinkedHashMap();
             	for(int z=0;z<sz;z++){
             		HashMap grpColMap=(HashMap) grpColList.get(z);
             		//log.info("grpColMap: "+grpColMap);
             		
                	map.put(grpColMap.get("itemName"),data.get(j).get(z));
                	
                }
             	
             	if(pivotCol!=null && !(pivotCol.isEmpty())){
             	map.put(pivotCol,columnsList[s+sz]);
             	}
             	
             	if(data.get(j).get(s+sz)!=null)
             	map.put(amtCol,data.get(j).get(s+sz));
                maps.add(map);
            }*/
        }
        sContext.close();
		return maps;
    }    
    
    
    /**
     * Author:Swetha
     * Account Balance Report generation
     * @param dataViewId
     * @param tenantId
     */
    @PostMapping("/accountBalanceReport")
    @Timed 
    public void accountingBalanceReport(@RequestParam Long dataViewId, @RequestParam Long tenantId){
    	
    	log.info("Rest Request to generate accountBalanceReport");
    	SparkSession spark = reportsService.getSparkSession();
    	JavaSparkContext sContext = new JavaSparkContext(spark.sparkContext());
    	SQLContext sqlCont = new SQLContext(sContext);
    	
    	String dbUrl = env.getProperty("spring.datasource.url");
		String[] parts = dbUrl.split("[\\s@&?$+-]+");
		String host = parts[0].split("/")[2].split(":")[0];
		String schemaName = parts[0].split("/")[3];
		String userName = env.getProperty("spring.datasource.username");
		String password = env.getProperty("spring.datasource.password");
		String jdbcDriver = env.getProperty("spring.datasource.jdbcdriver");
    	
    	/* Retrieve Unique combinations from Data View */
		HashMap map=reportsService.getGroupedDataViewData(dataViewId,spark,tenantId);
    	List<HashMap> aggDataList=(List<HashMap>) map.get("grpDataMaps");
    	String dataViewName=map.get("dataViewName").toString();
    	String[] aggDataCols=(String[]) map.get("aggDataCols");
    	String[] newAggDataArr=Arrays.copyOf(aggDataCols, aggDataCols.length-1);		
    	
    	Dataset<Row> recon_data_ds = sqlCont.read().format("jdbc")
				.option("url", dbUrl).option("user", userName)
				.option("password", password)
				.option("dbtable", "t_reconciliation_result").load().where("tenant_id=" + tenantId
						+ " and original_view_id=" + dataViewId).select("original_row_id");

		log.info("recon_data_ds.count(): " + recon_data_ds.count());
		
		List<Row> reconDataList=recon_data_ds.collectAsList();
		List<Long> originalRowIds=new ArrayList<Long>();
		for(int i=0;i<reconDataList.size();i++){
			Long orignalRowId=reconDataList.get(i).getLong(0);
			originalRowIds.add(orignalRowId);
		}
		
		log.info("originalRowIds sz for dataViewId: "+dataViewId+" is: "+originalRowIds.size());
		
		Dataset<Row> srcIdsDataset = sqlCont.read().format("jdbc")
				.option("url", dbUrl).option("user", userName)
				.option("password", password).option("dbtable", dataViewName).load();
		
		log.info("srcIdsDataset count:"+srcIdsDataset.count()); 
		srcIdsDataset.show();
				
		String srcIdDataListStr="";
		srcIdDataListStr=originalRowIds.toString();
		srcIdDataListStr=srcIdDataListStr.replace("[", "");
		srcIdDataListStr=srcIdDataListStr.replace("]", "");
		
		//log.info("srcIdDataListStr: "+srcIdDataListStr);
		
		Dataset<Row> recDvData=srcIdsDataset.where("scrIds in ("+srcIdDataListStr+")");
		
		log.info("reconciled dataview data count: "+recDvData.count());
		recDvData.show();
		
		
		for(int k=0;k<=aggDataList.size();k++){
			HashMap aggMap=aggDataList.get(k);
			log.info("aggMap: "+aggMap);
			BigDecimal additionsAmt=BigDecimal.ZERO;
			String subQry="";
			for(int h=0;h<newAggDataArr.length;h++){
				String colName=newAggDataArr[h];
				String colVal=aggMap.get(colName).toString();
				log.info("colName: "+colName+" colVal: "+colVal);
				subQry=subQry+colName+"='"+colVal+"'";
				if(h>=0 && h<=newAggDataArr.length-1){
					subQry=subQry+" and ";
				}
			}
			
			//log.info("subQry: "+subQry);
			if(subQry.endsWith(" and")){
		 		int lastAnd=subQry.lastIndexOf(" and");
		 		log.info("lastIndex of and: "+lastAnd);
		 		
		 		StringBuilder sb = new StringBuilder(subQry);
		 		sb.setCharAt(lastAnd, ' ');
		 		subQry = sb.toString();
		 		log.info("subQry after replacing and: "+subQry);
		 	}
		Dataset<Row> data = recDvData.where(subQry);
		log.info("data count after applying query: "+data.count());
		log.info("data: "+data);
		if(data!=null){
		Dataset<Row> amtData=data.select("Amount_57");
		log.info("amtData: "+amtData);
		List<Row> amtDataList=amtData.collectAsList();
		if(amtDataList!=null && !(amtDataList.isEmpty())){
		//log.info("amtDataList: "+amtDataList);
		//log.info("amtDataList get 0: "+amtDataList.get(0));
		//log.info("amtDataList get 0 n 0: "+amtDataList.get(0).get(0));
		additionsAmt=(BigDecimal) amtDataList.get(0).get(0);
		}
		}
		
		}
		
		sContext.close();
		
    }
    
    
    /**
     * @param tenantId
     * @param reportId
     * @param filtersMap
     * @return
     * @throws org.json.simple.parser.ParseException
     * @throws IOException
     * @throws OozieClientException
     * @throws URISyntaxException
     */
    
    @Async
    @PostMapping("/PivotViewReportAsync")
    @Timed
    public LinkedHashMap PivotViewReportAsync(HttpServletRequest request, @RequestParam Long reportId, @RequestBody(required = false) HashMap filtersMap) 
    		throws org.json.simple.parser.ParseException, IOException, OozieClientException, URISyntaxException{
    	
    	log.info("PivotViewReportAsync start time:"+ZonedDateTime.now());
    	LinkedHashMap map=new LinkedHashMap();
    	List<LinkedHashMap> reportReturnList=PivotViewReportNew(request, reportId, filtersMap);
    	map.put("status", "your request has been submitted");
    	log.info("PivotViewReportAsync end time:"+ZonedDateTime.now());
		return map;
    	
    }
    
    
    /**
     * Author: swetha
     * @param tenantId
     * @param reportId
     * @param filtersMap
     * @param pageNumber
     * @param pageSize
     * @param response
     * @return
     * @throws org.json.simple.parser.ParseException
     * @throws IOException
     * @throws OozieClientException
     * @throws URISyntaxException
     */
    
    @Async
    @PostMapping("/TabularViewReportGenerationAsync")
    @Timed
    public LinkedHashMap reportingPOCAsync(HttpServletRequest request, @RequestParam String reportId, @RequestBody(required = false) HashMap filtersMap, 
    		@RequestParam(required=false) Integer pageNumber, @RequestParam(required=false) Integer pageSize,HttpServletResponse response) throws org.json.simple.parser.ParseException, IOException, OozieClientException, URISyntaxException
    	{
    	log.info("TabularViewReportGenerationAsync start time:"+ZonedDateTime.now());
    	LinkedHashMap map=new LinkedHashMap();
    	//JSONObject jsonObj=reportsService.reportingPOC(tenantId, userId, reportId, filtersMap, pageNumber, pageSize, response);
    	JSONObject jsonObj=reportingPOC(request, reportId, filtersMap, pageNumber, pageSize, response);
    	map.put("status", "your request has been submitted");
    	log.info("TabularViewReportGenerationAsync end time:"+ZonedDateTime.now());
		return map;
    	}
    
    

    /**
     * Author: Swetha
     * @param request
     * @param reportId
     * @param filtersMap
     * @param pageNumber
     * @param pageSize
     * @param response
     * @return
     * @throws org.json.simple.parser.ParseException
     * @throws IOException
     * @throws OozieClientException
     * @throws URISyntaxException
     */
    @PostMapping("/TabularViewReportGeneration")
    @Timed
    public JSONObject reportingPOC(HttpServletRequest request, @RequestParam String reportId, @RequestBody(required = false) HashMap filtersMap, 
    		@RequestParam(required=false) Integer pageNumber, @RequestParam(required=false) Integer pageSize,HttpServletResponse response) throws org.json.simple.parser.ParseException, 
    		IOException, OozieClientException, URISyntaxException{
    	
    	HashMap map=userJdbcService.getuserInfoFromToken(request);
      	Long tenantId=Long.parseLong(map.get("tenantId").toString());
    	Long userId=Long.parseLong(map.get("userId").toString());
    		log.info("TabularViewReportGenerationAsync start time:"+ZonedDateTime.now());
        	
    		Reports repData=reportsRepository.findByTenantIdAndIdForDisplay(tenantId, reportId);
    		LinkedHashMap jobDataMap=reportsService.reportingPOC(tenantId, userId, repData.getId(), filtersMap, pageNumber, pageSize, response);
        	log.info("TabularViewReportGeneration end time:"+ZonedDateTime.now());
        	/*List<JobActions> jobActionsList=jobActionsRepository.findAll();
        	log.info("jobActionsList szoooooooooooooooooooo: "+jobActionsList.size());
    		log.info("jobActionsList gswde: "+jobActionsList);*/
        	ReportRequests repReq=(ReportRequests) jobDataMap.get("repReq");
         	JSONObject jsonObj=new JSONObject();
        	JSONObject output=new JSONObject();
        	 JSONObject pivotOutput=new JSONObject();
        	String outputPath="";
        	String pivotOutputPath="";
        	String lastOne="";
        	String val="";
        	String reportName="";
        	LinkedHashMap dataMap=new LinkedHashMap();
        	JSONObject newFileData=new JSONObject();
        	JSONObject newPivotFileData=new JSONObject();
        	//ReportRequests repReq=new ReportRequests();
        	String outputType="";
        	if(filtersMap!=null && !(filtersMap.isEmpty())){
        		outputType=filtersMap.get("outputType").toString();
        	}
        	if(jobDataMap!=null && !(jobDataMap.isEmpty())){
        	val=jobDataMap.get("jobId").toString();
        	log.info("tenantId: "+tenantId+" val: "+val+" userId: "+userId+" reportId: "+reportId);
        	//dataMap=reportsService.getStatus(tenantId, val,userId, reportId);
        	
        	 //LinkedHashMap dataMap=new LinkedHashMap();
    		 JSONObject taboutput=new JSONObject();
    		 HashMap requestInfo=new HashMap();
    		
    		List<JobActions> jobactList=jobActionsRepository.findByJobId(val);
    		log.info("jobactList sz: "+jobactList.size());
    		
    		 JobActions jobAction=jobActionsRepository.findReportOutputPath(val, tenantId);
    				log.info("jobAction: "+jobAction);
    				if(jobAction!=null){
    				log.info("jobAction: "+jobAction);
    				String actionName=jobAction.getActionName();
    				String[] actionNamesArr=actionName.split("is: ");
    				log.info("actionNamesArr -0: "+actionNamesArr[0]+" actionNamesArr-1: "+actionNamesArr[1]);
    				outputPath=actionNamesArr[1];
    				Long schedulerId=jobAction.getSchedulerId();
    				taboutput=reportsService.testFileReading(outputPath,userId,val,schedulerId,tenantId,repData.getId());
    				newFileData=(JSONObject) taboutput.clone();
    				newFileData.put("outputPath", outputPath);
    				//flag=true;
    				dataMap.put("output", taboutput);
    				dataMap.put("outputPath", outputPath);
    				}
    				else{
    					//output=null;
    				}
    				JobActions pivotPathData=jobActionsRepository.findReportPivoutOutputPath(val, tenantId);
    				String pivotPath="";
    				if(pivotPathData!=null){
    					log.info("pivotPathData: "+pivotPathData);
    					String actionName=pivotPathData.getActionName();
    					String[] actionNamesArr=actionName.split("is: ");
    					log.info("actionNamesArr -0: "+actionNamesArr[0]+" actionNamesArr-1: "+actionNamesArr[1]);
    					pivotPath=actionNamesArr[1];
    					Long schedulerId=pivotPathData.getSchedulerId();
    					pivotOutput=reportsService.testFileReading(pivotPath,userId,val,schedulerId,tenantId,repData.getId());
    					newPivotFileData=(JSONObject) pivotOutput.clone();
    					dataMap.put("pivotOutput", pivotOutput);
    					dataMap.put("pivotOutputPath", pivotPath);
    					}
    					else{
    						//for Account Analysis Report
    						dataMap.put("pivotOutputPath", outputPath);
    					}
    	
        	
			if(dataMap!=null && !(dataMap.isEmpty())){
				if(dataMap.containsKey("output")){
			output=(JSONObject) dataMap.get("output");//output
			if(dataMap.containsKey("outputPath")){
			log.info("dataMap.get(outputPath: "+dataMap.get("outputPath"));
			}
			else{
				log.info("dataMap doesn't contain outputPath");
			}
			outputPath=dataMap.get("outputPath").toString();
			pivotOutputPath=dataMap.get("pivotOutputPath").toString();
			lastOne=jobDataMap.get("lastOne").toString();
			reportName=jobDataMap.get("reportName").toString();
			//ReportRequests repReq=(ReportRequests) jobDataMap.get("repReq");
			log.info("repReq before appending to file object: "+repReq);
			//HashMap requestInfo=new HashMap();
			requestInfo.put("id", repReq.getId());
			requestInfo.put("request_type", repReq.getRequestType());
			requestInfo.put("req_name", repReq.getReqName());
			requestInfo.put("report_id", repReq.getReportId());
			requestInfo.put("status", repReq.getStatus());
			requestInfo.put("file_name", repReq.getFileName());
			requestInfo.put("output_path", repReq.getOutputPath());
			System.out.println("requestInfo; "+requestInfo);
			newFileData.put("requestInfo", requestInfo);
			newPivotFileData.put("requestInfo", requestInfo);
		}
			}
        	}
			
        	String status="";
		if(outputPath.length()>1)
		{	
		log.info("outputPath :"+outputPath);
		String[] bits = outputPath.split("/");
		lastOne = bits[bits.length-1];
		String[] pivotFileNameArr=pivotOutputPath.split("/");
		String pivotFileName=pivotFileNameArr[pivotFileNameArr.length-1];
		log.info("file name :"+lastOne);
		status=oozieService.getStatusOfOozieJobId(val);
		log.info("status after processess is completed :");
		repReq.setStatus(status);
		repReq.setGeneratedTime(ZonedDateTime.now());
		repReq.setOutputPath(outputPath);
		repReq.setPivotPath(pivotOutputPath);
		repReq.setFileName(lastOne);
		repReq.setLastUpdatedDate(ZonedDateTime.now());
		repReq=reportRequestsRepository.save(repReq);
		log.info(" final repReq :"+repReq);
		
		
		/* Logic to check if file has requestInfo: If not ovverride */
		/*if(output.containsKey("requestInfo")){
		}
		else{
			log.info("output doesnt' contain requestInfo key");
				 Write requestInfo to file 
				String cmpltFilePath=reportsService.FileReWriteHDFS(reportId,newFileData, "TABLE",lastOne,outputPath);
			} */
		/*if(pivotOutput.containsKey("requestInfo")){
		}
		else{
			log.info("pivotOutput doesnt' contain requestInfo key");
				 Write requestInfo to file 
				String cmpltFilePath=reportsService.FileReWriteHDFS(reportId,newPivotFileData, "PIVOT",pivotFileName,pivotOutputPath);
			}*/
		
		Notifications notification=new Notifications();
		notification.setModule("REPORTING");
		
		notification.setMessage("Requested "+reportName+" has been generated report");
		notification.setUserId(userId);
		notification.setIsViewed(false);
		//notification.setActionType("SCHEDULER");
		notification.setActionType("REQUEST,REPORT");
		//SchedulerDetails sch=schedulerDetailsRepository.findByOozieJobId(val);
		//notification.setActionValue(sch.getId().toString());
		log.info("reportId: "+reportId+" repReq.getId(): "+repReq.getIdForDisplay());
		String repIdReqId=repReq.getIdForDisplay().toString().concat(","+reportId.toString());
		notification.setActionValue(repIdReqId);
		notification.setTenantId(tenantId);
		notification.setCreatedBy(userId);
		notification.setCreationDate(ZonedDateTime.now());
		notification.setLastUpdatedBy(userId);
		notification.setLastUpdatedDate(ZonedDateTime.now());
		notification=notificationsRepository.save(notification);
		log.info("notification :"+notification);
		//}
		int totDataCnt=0;
		if(output.containsKey("X-COUNT")){
			log.info("output contains X-COUNT key");
		totDataCnt=Integer.parseInt(output.get("X-COUNT").toString());
		}
		else{
			log.info("output doesn't contains X-COUNT key");
		}
		log.info("totDataCnt: "+totDataCnt);
		response.addIntHeader("X-COUNT", totDataCnt);
		
		if(output.containsKey("data")){
			log.info("output contains key data");
		}
		else{
			log.info("output doesn't contains key data");
		}
		List<LinkedHashMap> maps=(List<LinkedHashMap>) output.get("data");
		//log.info("maps final count: "+maps.size());
		List<LinkedHashMap> subMaps=new ArrayList<LinkedHashMap>();
		if(maps.size()>0){
			if(maps.size()>=25){
			subMaps=maps.subList(0, 25);
			}
			else if(maps.size()<=25)
				subMaps=maps.subList(0, (maps.size()));
		}
		log.info("submaps final count: "+subMaps.size());
		
		
		output.put("data", subMaps);
		JSONObject requestInfo=(JSONObject) output.get("requestInfo");
		log.info("requestInfo from output: "+requestInfo);
		output.put("requestInfo", repReq);
		
		String path=dataMap.get("outputPath").toString();//outputPath
		log.info("path: "+path);
		
		output.put("outputPath", path);
		}
		
		else{
			log.info("In output path doesnt exists case and Updating Request status");
			status=oozieService.getStatusOfOozieJobId(val);
			log.info("status: "+status);
			repReq.setStatus(status);
			repReq.setLastUpdatedDate(ZonedDateTime.now());
			repReq=reportRequestsRepository.save(repReq);
			log.info(" final repReq :"+repReq);
			
		}
		log.info("**end of tabular API** "+ZonedDateTime.now());
		
		log.info("outputType: "+outputType);
		if(outputType.equalsIgnoreCase("TABLE")){
			return output;
		}
		else 
			return newPivotFileData;
			
    }
    
    
    /**
     * Author: Swetha
     * @param dataViewId
     * @param tenantId
     * @return
     */
    @GetMapping("/getGrpData")
    @Timed
    public List<LinkedHashMap> getGrpData(@RequestParam Long dataViewId, @RequestParam Long tenantId){
    	SparkSession spark=reportsService.getSparkSession();
    HashMap map=reportsService.getGroupedDataViewData(dataViewId, spark, tenantId);
    log.info("map: "+map);
    List<LinkedHashMap> grpDta=(List<LinkedHashMap>) map.get("grpDataMaps");
    return grpDta;
    }
    
    /**
     * Author: Swetha
     * @param fileLocation
     * @param request
     * @return
     * @throws IOException
     */
    @GetMapping("/deleteHdfsFile")
    @Timed
    public boolean deleteFileFromHDFS(String fileLocation,HttpServletRequest request)
    		  throws IOException {
    		 /*
    		  * fully qualified name = HDFS location(ip address + port) +
    		  * fileLocation hdfs://192.168.213.133:54310/<fileLocation>
    		  */
    	
    	HashMap map0=userJdbcService.getuserInfoFromToken(request);
		Long tenantId=Long.parseLong(map0.get("tenantId").toString());
    	TenantConfig tenConfig=tenantConfigRepository.findByTenantIdAndKeyAndMeaning(tenantId,"HDFS Host Path","HDFS Properties");
    		 String pathString = tenConfig.getValue()+"/" + fileLocation;

    		 // Create configuration object - get config files from class path
    		 Configuration conf = new Configuration();

    		 /*
    		  * Add configuration file core-site.xml to configuration object
    		  * core-site.xml is available in <Hadoop_HOME>/conf/core-site.xml
    		  */
    		/* conf.addResource(new Path(
    		   "/usr/local/hadoop2.6.1/etc/hadoop/core-site.xml"));*/

    		 FileSystem fs = null;
    		 boolean status = false;
    		 try {
    		  // create a FileSystem object passing configuration object config
    		  fs = FileSystem.get(conf);

    		  // Create path object and check for its existence
    		  Path path = new Path(pathString);
    		  if (fs.exists(path)) {
    		   // false indicates do not deletes recursively
    		   status = fs.delete(path, false);
    		   
    		  } else {
    		   System.out.println("File does not exist on HDFS");
    		   status = false;
    		  }

    		 } catch (Exception e) {
    		  e.printStackTrace();
    		 } finally {
    		  // Close file descriptors
    		  if (fs != null)
    		   fs.close();
    		 }
    		 return status;
    		}
    
    
    /**
     * Author: Swetha
     * @param tenantId
     * @param reportId
     * @param filtersMap
     * @param pageNumber
     * @param pageSize
     * @param response
     * @return
     * @throws AnalysisException
     * @throws IOException
     * @throws ParseException
     * @throws ClassNotFoundException
     * @throws SQLException
     */
    @PostMapping("/accountBalanceReportTesting")
    @Timed
    public JSONObject accountBalanceReport(
			@RequestParam Long tenantId, @RequestParam Long reportId,
			@RequestBody(required = false) HashMap filtersMap,  @RequestParam(required = false) Integer pageNumber,@RequestParam(required = false) Integer pageSize,HttpServletResponse response) throws AnalysisException,
			IOException, ParseException, ClassNotFoundException, SQLException {
    	
    	log.info("in accountBalanceReport generation start at: "+new Date());
    	
    	Reports reports=reportsRepository.findOne(reportId);
    	Long dvId=reports.getSourceViewId();
    	
    	List<ReportDefination> reportCondList=reportDefinationRepository.fetchReportConditions(reportId);
    	log.info("reportCondList sz: "+reportCondList.size());
    	String condQry = "";
    	for(int i=0;i<reportCondList.size();i++){
    		
    		String conSubQry = "";
    		ReportDefination repCond=reportCondList.get(i);
    		Long refColId=repCond.getRefColId();
    		String condOperOrg=repCond.getConditionalOperator();
    		String condVal=repCond.getConditionalVal();
    		String condOp = "";
    		
    		DataViewsColumns dvc=dataViewsColumnsRepository.findOne(refColId);
    		String dvColId=dvc.getRefDvColumn();
    		String colDtType=dvc.getColDataType();
    		
    		FileTemplateLines ftl=fileTemplateLinesRepository.findOne(Long.parseLong(dvColId));
    		String colAlias=ftl.getColumnAlias();
    		
    		if (colDtType.equalsIgnoreCase("VARCHAR")) {
				if (condOperOrg.equalsIgnoreCase("EQUALS")
						|| condOperOrg.equalsIgnoreCase("NOT EQUALS")) {
					condOp = "=";
					conSubQry = conSubQry + colAlias + condOp + "'" + condVal
							+ "'";
				}
				if (condOperOrg.equalsIgnoreCase("CONTAINS")) {
					condOp = "like";
					conSubQry = conSubQry + colAlias + condOp + "'%" + condVal
							+ "%'";
				}
				if (condOperOrg.equalsIgnoreCase("BEGINS_WITH")) {
					condOp = "like";
					conSubQry = conSubQry + colAlias + condOp + "'%" + condVal
							+ "'";
				}
				if (condOperOrg.equalsIgnoreCase("ENDS_WITH")) {
					condOp = "like";
					conSubQry = conSubQry + colAlias + condOp + "'" + condVal
							+ "%'";
				}
			} else if (colDtType.equalsIgnoreCase("INTEGER")) {
				conSubQry = conSubQry + colAlias + condOperOrg + condVal;
				// handle between
			} else if (colDtType.equalsIgnoreCase("DATE")) {
				// handle between
				conSubQry = conSubQry + colAlias + condOperOrg + "'" + condVal
						+ "'";
			} else if (colDtType.equalsIgnoreCase("DATETIME")) {
				// handle between
				conSubQry = conSubQry + colAlias + condOperOrg + "'" + condVal
						+ "'";
			} else if (colDtType.equalsIgnoreCase("DECIMAL")) {
				// handle between
				conSubQry = conSubQry + colAlias + condOperOrg + condVal;
			}

			condQry = condQry + conSubQry;
			if (i >= 0 && i < reportCondList.size() - 1 && i != reportCondList.size()) {
				condQry = condQry + ",";
			}
    		
    	}
    	log.info("final condQry: " + condQry);
    	
    	
    	HashMap balancesInfo=reportsService.getBalancesQuery(dvId,reportId);
    	String query = balancesInfo.get("query").toString();
    	String updQuery = " select "+query+" from t_balance_type";
    	log.info("updQuery: "+updQuery);
    	List<String> layoutColList = (List<String>) balancesInfo.get("layoutColList");
    	log.info("layoutColList in balances report: "+layoutColList);
    	
    	String finQuery = "";
    	
    	if (filtersMap != null && !(filtersMap.isEmpty())) {
			
			String conQuery = " where ";
			String conSubQueryFin = "";
				log.info("filters exists");
				if (filtersMap.containsKey("fields")) {
					List<HashMap> filtersList = (List<HashMap>) filtersMap
							.get("fields");
					log.info("fields exists with sz: " + filtersList.size());
					for (int i = 0; i < filtersList.size(); i++) {
						HashMap filterMap = filtersList.get(i);
						log.info("filterMap: " + filterMap);
						String conSubQuery = "";
						String colName="";
						Long refColId = Long.parseLong(filterMap.get(
								"refColId").toString());
						Long refSrcId=Long.parseLong(filterMap.get(
								"rParamSrcId").toString());
						String refType=filterMap.get(
								"refType").toString();
						log.info("refColId: "+refColId+" refSrcId: "+refSrcId+" refType: "+refType);
						/*Dataset<Row> repDefParamdataDS =repDefConddataDF.load()
								.where("report_id="
										+ reportId
										+ " and ref_type_id='"+refType+"' and ref_src_id="+refSrcId+" and ref_col_id="+refColId)
										.select("display_name").cache();*/
						ReportDefination repDefParamData=reportDefinationRepository.findByReportIdAndRefTypeIdAndRefSrcIdAndRefColId(reportId, refType, refSrcId, refColId);
						ReportParameters repParamData=reportParametersRepository.findByReportIdAndRefTypeidAndRefSrcIdAndRefColId(reportId, refType, refSrcId, refColId);
						
							if(repDefParamData!=null){
								colName=repDefParamData.getDisplayName();
								//log.info("colName from dataview: "+colName);
							}
							else{
								colName=repParamData.getDisplayName();
								//log.info("colName rep param: "+colName);
							}
						//log.info("colName: "+colName);
						String selType = filterMap.get("fieldType").toString();
						//log.info("selType: " + selType);
						if (selType.equalsIgnoreCase("MULTI_SELECT_LOV")
								|| selType
										.equalsIgnoreCase("SINGLE_SELECT_LOV")
								|| selType.equalsIgnoreCase("SINGLE_SELECTION")
								|| selType.equalsIgnoreCase("TEXT")) {
							String blankQuery="";
							String selValData = "";
							String selValVar = filterMap.get("selectedValue")
									.toString();
							log.info("selValVar: " + selValVar);
							if(selValVar!=null && !(selValVar.isEmpty()) && !(selValVar.equalsIgnoreCase("[]"))){
							selValVar = selValVar.replace("[", "");
							selValVar = selValVar.replace("]", "");
							List<String> selValList = new ArrayList<String>(
									Arrays.asList(selValVar.split(",")));
							for (int k = 0; k < selValList.size(); k++) {
								String selVal = selValList.get(k);
								selVal = selVal.trim();
								if(selVal.equalsIgnoreCase("(Blank)")){
									blankQuery="`"+colName+"` is null";
									log.info("blankQuery: "+blankQuery);
								}
								else selValData = selValData + "'" + selVal + "'";
								if (k >= 0 && k < selValList.size() - 1) {
									if(selValData!=null && !(selValData.isEmpty()))
									selValData = selValData + ",";
								}
							}
							if (selValData != null && !(selValData.isEmpty())) {
								if (selValData.endsWith(",")) {
									int indx = selValData.lastIndexOf(",");
									selValData = selValData.substring(0, indx);
									log.info("selValData after removing comma: "
											+ selValData);
								}
								conSubQuery = conSubQuery + "`"+colName +"`" + " in ("
										+ selValData + ") ";
							}
							else{
								
							}
							if(selValList.size()>1 && blankQuery!=null && !(blankQuery.isEmpty())){
								conSubQuery=" ("+conSubQuery+" or "+blankQuery+") ";//(goup_by in (1) or goup_by is null)
							}
							if(conSubQuery==null || (conSubQuery!=null && conSubQuery.isEmpty()) || conSubQuery.equalsIgnoreCase("")){
								if(blankQuery!=null && !(blankQuery.isEmpty()))
								conSubQuery=blankQuery;
							}
							log.info("final conSubQuery: "+conSubQuery);
							}
						} else if (selType.equalsIgnoreCase("AUTO_COMPLETE")) {
							String selVal = filterMap.get("selectedValue")
									.toString();
							if (selVal != null && !(selVal.isEmpty())) {
								conSubQuery = conSubQuery + "`"+colName +"`" + " in ('"
										+ selVal + "') ";
							}
						} else if (selType
								.equalsIgnoreCase("BOOLEAN_SELECTION")) {
							String selVal = filterMap.get("selectedValue")
									.toString();
							if (selVal != null && !(selVal.isEmpty())) {
								if (conSubQuery.equalsIgnoreCase(" where ")) {

								} else
									conSubQuery = " and ";
								conSubQuery = conSubQuery + "`"+colName +"`" + " is "
										+ selVal;
							}
						} else if (selType.equalsIgnoreCase("AMOUNT_RANGE")) {
							String map = filterMap.get("selectedValue")
									.toString();
							if (map != null && !(map.isEmpty())) {
								log.info("map: " + map);
								map = map.replace("{fromValue=", "");
								map = map.replace("toValue=", "");
								log.info("map aftr replace: " + map);
								String arr[] = map.split(",");
								log.info(" arr[0]: " + arr[0]);
								log.info("arr[1]: " + arr[1]);
								arr[1] = arr[1].replace("}", "");
								log.info("arr[1]: " + arr[1]
										+ " arr[0]: " + arr[0]);
								String fromValue = arr[0].trim();
								String toValue = arr[1].trim();
								conSubQuery = conSubQuery + "`"+colName +"`"
										+ " between " + fromValue + " and "
										+ toValue;
							}
						}
						log.info("conSubQuery: " + conSubQuery);
						if (conSubQuery.length() > 1) {
							conSubQuery = conSubQuery + " and ";
						}
						conSubQueryFin = conSubQuery;
						if (conSubQuery.equalsIgnoreCase(" where ")) {

						} else {
							finQuery = finQuery + conSubQuery;
						}
					}

				}
				
				log.info("finQuery after conQuery: " + finQuery);
				String a = "";
				if (finQuery.endsWith(" and ")) {
					int lastAndVal = finQuery.lastIndexOf(" and ");
					log.info("lastAndVal :" + lastAndVal);

					StringBuilder sb = new StringBuilder(finQuery);
					finQuery = sb.substring(0, lastAndVal);
					log.info("part2 after replacing : " + finQuery);
				}
				log.info("finQuery after framing filters: " + finQuery);
				log.info("finQuery after applying status filters: " + finQuery);
				//data = data.filter(finQuery);
				//log.info("Previewing filtered data with sz: "+data.count());
		}else {
			log.info("filters doesnt exist");
		}
    	
    	/*Query distinctList=em.createQuery("select "+ query+ " FROM BalanceType");
		log.info("distinctList : "+distinctList);
		List distinct = new ArrayList<String>();
		distinct =distinctList.getResultList();
		
		log.info("distinct: "+distinct);*/
    	
    	String dbUrl=env.getProperty("spring.datasource.url");
		String[] parts=dbUrl.split("[\\s@&?$+-]+");
		String host = parts[0].split("/")[2].split(":")[0];
		String schemaName=parts[0].split("/")[3];
		String userName = env.getProperty("spring.datasource.username");
		String password = env.getProperty("spring.datasource.password");
		String jdbcDriver = env.getProperty("spring.datasource.jdbcdriver");
    	   
    	   Connection conn = null;
    	   Statement stmt = null;
    	   Statement stmt2 = null;
		   ResultSet result = null;
		   ResultSet rs=null;
    	   List<LinkedHashMap> mapList2=new ArrayList<LinkedHashMap>();
		try{
    	      Class.forName(jdbcDriver);
    	      conn = DriverManager.getConnection(dbUrl, userName, password);
    	      log.info("Connected database successfully...");
    	      stmt = conn.createStatement();
    	      
    	 stmt2 = conn.createStatement();
 	     String count = null;
 	   result=stmt2.executeQuery(updQuery);
 	     
 	   rs=stmt2.getResultSet();
 	
 	  ResultSetMetaData rsmd2 = result.getMetaData();
 	  log.info("col count: "+rsmd2.getColumnCount());
 	int columnsNumber = rsmd2.getColumnCount();
 	log.info("columnsNumber: "+columnsNumber);
 	int columnCount = rsmd2.getColumnCount();
 	while(rs.next()){
 		LinkedHashMap<String,String> map2=new LinkedHashMap<String,String>();
   	for (int i = 1; i <= columnCount; i++ ) {
			  //String name = rsmd2.getColumnName(i); 
			  //log.info("name: "+name);
			  String alias=rsmd2.getColumnLabel(i);
			 // log.info("alias: "+alias);
   	 for(int t=0,num=1;t<columnsNumber;t++, num++){ 
   		 String Val=rs.getString(num);
   	 }
   	 map2.put(alias, rs.getString(i));
   	}
   	mapList2.add(map2);
 	}
	   }catch(SQLException se){
		   log.info("se: "+se);
      }
		finally{
			result.close();
			rs.close();
			stmt.close();
			stmt2.close();
			conn.close();
		}
		
		int limit = 0;
		if(pageNumber == null || pageNumber == 0)
		{
			pageNumber = 0;
		}
		if(pageSize == null || pageSize == 0)
		{
			pageSize = mapList2.size();
		}
			limit = ((pageNumber+1) * pageSize + 1)-1;
		int startIndex=pageNumber*pageSize; 
		
		if(limit>mapList2.size()){
			limit=mapList2.size();
		}
		
		log.info("startIndex: "+startIndex+" limit: "+limit);
		
		List<LinkedHashMap> output=	mapList2.subList(startIndex, limit);
		
		JSONObject obj=new JSONObject();
		
		log.info("mapList2.size(): "+mapList2.size());
		response.addIntHeader("X-COUNT", mapList2.size());
		
		 List<LinkedHashMap> headerMap=new ArrayList<LinkedHashMap>();
		 if(output!=null && !(output.isEmpty()) && output.get(0)!=null && !(output.get(0).isEmpty())){
		 headerMap=reportsService.tabuleHeaderData(output.get(0), reportId); 
		 log.info("headerMap: "+headerMap);
		 }
		
		obj.put("data", output);
		obj.put("columns", headerMap);
		
	return obj;
    }
    
    /**
     * Author: Swetha
     * Api to present report output pagewise by reading hdfs file
     * @param reportPath
     * @param pageNumber
     * @param pageSize
     * @param response
     * @return
     * @throws IOException
     * @throws URISyntaxException
     * @throws org.json.simple.parser.ParseException
     */
    @GetMapping("/getReportOutputByPage")
    public JSONObject getReportOutputByPage(@RequestParam String reportPath, @RequestParam(required=false) Integer pageNumber, @RequestParam(required=false) Integer pageSize,HttpServletResponse response) throws IOException, URISyntaxException, org.json.simple.parser.ParseException{
    	
    	log.info("Rest Request to getReportOutputByPage with parameters: ");
    	log.info("reportPath: "+reportPath);
    	log.info("pageNumber: "+pageNumber+" & pageSize: "+pageSize);
    	
    	JSONObject finalOutput=new JSONObject();
    	List<JSONObject> limitedOutputList=new ArrayList<JSONObject>();
    	
    	LinkedHashMap cmpltOutput=reportsService.hdfsFileReading(reportPath);
    	int totDataCnt=Integer.parseInt(cmpltOutput.get("X-COUNT").toString());
		log.info("totDataCnt: "+totDataCnt);
		response.addIntHeader("X-COUNT", totDataCnt);
    	
		//LinkedHashMap outputMap=cmpltOutput.get("")
		
		List<JSONObject> outputList=(List<JSONObject>) cmpltOutput.get("data");
		//log.info("outputList: "+outputList);
		List<JSONObject> HeaderList=(List<JSONObject>) cmpltOutput.get("columns");
		log.info("HeaderList: "+HeaderList);
		
		JSONObject hMap=HeaderList.get(0);
		log.info("hMap: "+hMap);
		
		int limit = 0;
		if(pageNumber == null || pageNumber == 0)
		{
			pageNumber = 0;
		}
		if(pageSize == null || pageSize == 0)
		{
			pageSize = totDataCnt;
		}
			limit = ((pageNumber+1) * pageSize + 1)-1;
		int startIndex=pageNumber*pageSize; 
		
		if(limit>totDataCnt){
			limit=totDataCnt;
		}
		
		log.info("startIndex: "+startIndex+" limit: "+limit);
		
		for(int j=startIndex;j<limit;j++){
			
			JSONObject map=outputList.get(j);
			limitedOutputList.add(map);
			
		}
		
		log.info("limitedOutputList: "+limitedOutputList);
		finalOutput.put("data", limitedOutputList);
		finalOutput.put("columns", HeaderList);
		finalOutput.put("X-COUNT", totDataCnt);
		return finalOutput;
    }
    
    
    
    /**
     * Author: Ravali,
     * Author: Swetha [Integrated Global & Column level search]
     * @param requestId
     * @param outputType
     * @param pageNumber
     * @param pageSize
     * @param sortColumn
     * @param sortOrder
     * @param searchString
     * @param searchColumn
     * @param response
     * @param request
     * @return
     * @throws IOException
     * @throws URISyntaxException
     * @throws org.json.simple.parser.ParseException
     * @throws JSONException
     * @throws ParseException
     */
   /* @PostMapping("/getReportOutputByRequestId")
    public JSONObject getReportOutputByRequestId(@RequestParam String requestId,@RequestParam String outputType, @RequestParam(required=false) Integer pageNumber, @RequestParam(required=false) Integer pageSize,@RequestParam(required=false) String sortColumn
    		,@RequestParam(required=false) String sortOrder,@RequestParam(required=false) String searchString, @RequestBody(required=false) List<HashMap> searchObject,
    		HttpServletResponse response, HttpServletRequest request) throws IOException, URISyntaxException, org.json.simple.parser.ParseException, JSONException, ParseException{

    	log.info("Rest Request to getReportOutputByPage with parameters requestId: "+requestId+"sortColumn :"+sortColumn+" sortOrder:"+sortOrder+" searchString :"+searchString+" outputType :"+outputType);
    	log.info("requestId: "+requestId);
    	HashMap map1=userJdbcService.getuserInfoFromToken(request);
    	Long tenantId=Long.parseLong(map1.get("tenantId").toString());
    	ReportRequests reqData=reportRequestsRepository.findByTenantIdAndIdForDisplay(tenantId, requestId);
    	ReportRequests req=reportRequestsRepository.findOne(reqData.getId());
    	System.out.println("req: "+req);
    	String outputPath="";
    	if(outputType.equalsIgnoreCase("table"))
    		outputPath=req.getOutputPath();
    	else if(outputType.equalsIgnoreCase("pivot"))
    		outputPath=req.getPivotPath();
    	log.info("outputPath: "+outputPath);
    	log.info("pageNumber: "+pageNumber+" & pageSize: "+pageSize);

    	JSONObject finalOutput=new JSONObject();
    	List<JSONObject> limitedOutputList=new ArrayList<JSONObject>();

    	LinkedHashMap cmpltOutput=reportsService.hdfsFileReading(outputPath);
    	int totDataCnt=0;
    	List<JSONObject> outputList=new ArrayList<JSONObject>();
    	if(cmpltOutput.get("X-COUNT")!=null)
    	{
    		totDataCnt=Integer.parseInt(cmpltOutput.get("X-COUNT").toString());
    	log.info("totDataCnt: "+totDataCnt);
    	}

    	if((sortColumn!=null && sortOrder!=null && searchString==null)|| (sortColumn!=null && sortOrder==null && searchString==null))
    	{
    		log.info("in 1");
    		finalOutput=reportsService.sortingValuesInJson(cmpltOutput, sortColumn, sortOrder, pageNumber, pageSize);
    		response.addIntHeader("X-COUNT", totDataCnt);
    		outputList=(List<JSONObject>) finalOutput.get("data");
    	}
    	else if(sortColumn==null && sortOrder==null && (searchString!=null || (searchObject!=null && !(searchObject.isEmpty()))))
    	{
    		log.info("in 2");
    		finalOutput=reportsService.SearchValuesInReportOutputJson123(cmpltOutput, searchString, pageNumber, pageSize,null,null,searchObject);
    		response.addIntHeader("X-COUNT", Integer.valueOf(finalOutput.get("totalCount").toString()));
    		outputList=(List<JSONObject>) finalOutput.get("data");
    	}
    	else if((sortColumn!=null && sortOrder!=null && searchString!=null) ||(sortColumn!=null && sortOrder==null && (searchString!=null || searchObject!=null && !(searchObject.isEmpty()))))
    	{
    		log.info("in 3");
    		finalOutput=reportsService.SearchValuesInReportOutputJson123(cmpltOutput, searchString, pageNumber, pageSize,sortColumn,sortOrder,searchObject);
    		response.addIntHeader("X-COUNT", Integer.valueOf(finalOutput.get("totalCount").toString()));
    		outputList=(List<JSONObject>) finalOutput.get("data");
    	}
    	else
    	{
    		log.info("in 4");
    		response.addIntHeader("X-COUNT", totDataCnt);
    		if(outputType.equalsIgnoreCase("pivot"))
    			outputList=(List<JSONObject>) cmpltOutput.get("data");
    		else
    			outputList=(List<JSONObject>) cmpltOutput.get("data");
    	}
    		List<JSONObject> HeaderList=new ArrayList<JSONObject>();
    		if(cmpltOutput.containsKey("columns")){
    		HeaderList=(List<JSONObject>) cmpltOutput.get("columns");
    		}
    		log.info("HeaderList: "+HeaderList);
    		int limit = 0;
    		if(pageNumber == null || pageNumber == 0)
    		{
    			pageNumber = 0;
    		}
    		if(pageSize == null || pageSize == 0)
    		{
    			pageSize = totDataCnt;
    		}
    		limit = ((pageNumber+1) * pageSize + 1)-1;
    		int startIndex=pageNumber*pageSize; 

    		if(limit>totDataCnt){
    			limit=totDataCnt;
    		}

    		log.info("startIndex: "+startIndex+" limit: "+limit);
    		
    		limitedOutputList=outputList.subList(startIndex, limit);
    		finalOutput.put("data", limitedOutputList);
    		if(outputType.equalsIgnoreCase("pivot"))
    			finalOutput.put("data", outputList);
    		finalOutput.put("columns", HeaderList);
    		finalOutput.put("X-COUNT", totDataCnt);
    	//}
    	return finalOutput;
    }*/
    
    @PostMapping("/getReportOutputByRequestId")
    public JSONObject getReportOutputByRequestId(@RequestParam String requestId,@RequestParam String outputType, @RequestParam(required=false) Integer pageNumber, @RequestParam(required=false) Integer pageSize,@RequestParam(required=false) String sortColumn
    		,@RequestParam(required=false) String sortOrder,@RequestParam(required=false) String searchString, @RequestBody(required=false) List<HashMap> searchObject,
    		HttpServletResponse response, HttpServletRequest request) throws IOException, URISyntaxException, org.json.simple.parser.ParseException, JSONException, ParseException{

    	log.info("Rest Request to getReportOutputByPage with parameters requestId: "+requestId+"sortColumn :"+sortColumn+" sortOrder:"+sortOrder+" searchString :"+searchString+" outputType :"+outputType);
    	log.info("requestId: "+requestId);
    	HashMap map1=userJdbcService.getuserInfoFromToken(request);
    	Long tenantId=Long.parseLong(map1.get("tenantId").toString());
    	ReportRequests reqData=reportRequestsRepository.findByTenantIdAndIdForDisplay(tenantId, requestId);
    	ReportRequests req=reportRequestsRepository.findOne(reqData.getId());
    	System.out.println("req: "+req);
    	String outputPath="";
    	if(outputType.equalsIgnoreCase("table"))
    		outputPath=req.getOutputPath();
    	else if(outputType.equalsIgnoreCase("pivot"))
    		outputPath=req.getPivotPath();
    	log.info("outputPath: "+outputPath);
    	log.info("pageNumber: "+pageNumber+" & pageSize: "+pageSize);

    	JSONObject finalOutput=new JSONObject();
    	List<JSONObject> limitedOutputList=new ArrayList<JSONObject>();

    	LinkedHashMap cmpltOutput=reportsService.hdfsFileReading(outputPath);
    	int totDataCnt=0;
    	if(cmpltOutput.get("X-COUNT")!=null)
    	{
    		totDataCnt=Integer.parseInt(cmpltOutput.get("X-COUNT").toString());
    	log.info("totDataCnt: "+totDataCnt);
    	}
    	

    	if((sortColumn!=null && sortOrder!=null && searchString==null)|| (sortColumn!=null && sortOrder==null && searchString==null))
    	{
    		log.info("in 1");
    		finalOutput=reportsService.sortingValuesInJson(cmpltOutput, sortColumn, sortOrder, pageNumber, pageSize);
    		response.addIntHeader("X-COUNT", totDataCnt);
    	}
    	else if(sortColumn==null && sortOrder==null && (searchString!=null || (searchObject!=null && !(searchObject.isEmpty()))))
    	{
    		log.info("in 2");
    		finalOutput=reportsService.SearchValuesInReportOutputJson123(cmpltOutput, searchString, pageNumber, pageSize,null,null,searchObject);
    		log.info("finalOutput totalCount in 2: "+finalOutput.get("totalCount").toString());
    		response.addIntHeader("X-COUNT", Integer.valueOf(finalOutput.get("totalCount").toString()));
    	}
    	else if((sortColumn!=null && sortOrder!=null && searchString!=null) ||(sortColumn!=null && sortOrder==null && (searchString!=null || searchObject!=null && !(searchObject.isEmpty()))))
    	{
    		log.info("in 3");
    		finalOutput=reportsService.SearchValuesInReportOutputJson123(cmpltOutput, searchString, pageNumber, pageSize,sortColumn,sortOrder,searchObject);
    		response.addIntHeader("X-COUNT", Integer.valueOf(finalOutput.get("totalCount").toString()));
    	}
    	else
    	{
    		log.info("in 4");
    		response.addIntHeader("X-COUNT", totDataCnt);
    		List<JSONObject> outputList=new ArrayList<JSONObject>();
    		if(outputType.equalsIgnoreCase("pivot"))
    			outputList=(List<JSONObject>) cmpltOutput.get("data");
    		else
    			outputList=(List<JSONObject>) cmpltOutput.get("data");
    		List<JSONObject> HeaderList=new ArrayList<JSONObject>();
    		if(cmpltOutput.containsKey("columns")){
    		HeaderList=(List<JSONObject>) cmpltOutput.get("columns");
    		}
    		log.info("HeaderList: "+HeaderList);
    		int limit = 0;
    		if(pageNumber == null || pageNumber == 0)
    		{
    			pageNumber = 0;
    		}
    		if(pageSize == null || pageSize == 0)
    		{
    			pageSize = totDataCnt;
    		}
    		limit = ((pageNumber+1) * pageSize + 1)-1;
    		int startIndex=pageNumber*pageSize; 

    		if(limit>totDataCnt){
    			limit=totDataCnt;
    		}

    		log.info("startIndex: "+startIndex+" limit: "+limit);
    		
    		limitedOutputList=outputList.subList(startIndex, limit);
    		finalOutput.put("data", limitedOutputList);
    		if(outputType.equalsIgnoreCase("pivot"))
    			finalOutput.put("data", outputList);
    		finalOutput.put("columns", HeaderList);
    		finalOutput.put("X-COUNT", totDataCnt);
    	}
    	log.info("final finalOutput sz: "+finalOutput.size());
    	return finalOutput;
    }
    /*@GetMapping("/getReportOutputByRequestId")
    public JSONObject getReportOutputByRequestId(@RequestParam String requestId,@RequestParam String outputType, @RequestParam(required=false) Integer pageNumber, @RequestParam(required=false) Integer pageSize,@RequestParam(required=false) String sortColumn
    		,@RequestParam(required=false) String sortOrder,@RequestParam(required=false) String searchString, @RequestParam(required=false) String searchColumn,
    		HttpServletResponse response, HttpServletRequest request) throws IOException, URISyntaxException, org.json.simple.parser.ParseException, JSONException, ParseException{

    	log.info("Rest Request to getReportOutputByPage with parameters requestId: "+requestId+"sortColumn :"+sortColumn+" sortOrder:"+sortOrder+" searchString :"+searchString+" outputType :"+outputType);
    	log.info("requestId: "+requestId);
    	HashMap map1=userJdbcService.getuserInfoFromToken(request);
    	Long tenantId=Long.parseLong(map1.get("tenantId").toString());
    	ReportRequests reqData=reportRequestsRepository.findByTenantIdAndIdForDisplay(tenantId, requestId);
    	ReportRequests req=reportRequestsRepository.findOne(reqData.getId());
    	System.out.println("req: "+req);
    	String outputPath="";
    	log.info("outputType: "+outputType);
    	if(outputType.equalsIgnoreCase("table"))
    		outputPath=req.getOutputPath();
    	else if(outputType.equalsIgnoreCase("pivot"))
    		outputPath=req.getPivotPath();
    	log.info("outputPath: "+outputPath);
    	log.info("pageNumber: "+pageNumber+" & pageSize: "+pageSize);

    	JSONObject finalOutput=new JSONObject();
    	List<JSONObject> limitedOutputList=new ArrayList<JSONObject>();

    	LinkedHashMap cmpltOutput=reportsService.hdfsFileReading(outputPath);
    	int totDataCnt=0;
    	if(cmpltOutput.get("X-COUNT")!=null)
    	{
    		totDataCnt=Integer.parseInt(cmpltOutput.get("X-COUNT").toString());
    	log.info("totDataCnt: "+totDataCnt);
    	}
    	

    	if((sortColumn!=null && sortOrder!=null && searchString==null)|| (sortColumn!=null && sortOrder==null && searchString==null))
    	{
    		log.info("in 1");
    		finalOutput=reportsService.sortingValuesInJson(cmpltOutput, sortColumn, sortOrder, pageNumber, pageSize);
    		response.addIntHeader("X-COUNT", totDataCnt);
    	}
    	else if(sortColumn==null && sortOrder==null && searchString!=null)
    	{
    		log.info("in 2");
    		finalOutput=reportsService.SearchValuesInReportOutputJson(cmpltOutput, searchString, pageNumber, pageSize,null,null,searchColumn);
    		response.addIntHeader("X-COUNT", Integer.valueOf(finalOutput.get("totalCount").toString()));
    	}
    	else if((sortColumn!=null && sortOrder!=null && searchString!=null) ||(sortColumn!=null && sortOrder==null && searchString!=null))
    	{
    		log.info("in 3");
    		finalOutput=reportsService.SearchValuesInReportOutputJson(cmpltOutput, searchString, pageNumber, pageSize,sortColumn,sortOrder,searchColumn);
    		response.addIntHeader("X-COUNT", Integer.valueOf(finalOutput.get("totalCount").toString()));
    	}
    	else
    	{
    		log.info("in 4");
    		response.addIntHeader("X-COUNT", totDataCnt);
    		List<JSONObject> outputList=new ArrayList<JSONObject>();
    		if(outputType.equalsIgnoreCase("pivot"))
    			outputList=(List<JSONObject>) cmpltOutput.get("data");
    		else
    			outputList=(List<JSONObject>) cmpltOutput.get("data");
    		List<JSONObject> HeaderList=new ArrayList<JSONObject>();
    		if(cmpltOutput.containsKey("columns")){
    		HeaderList=(List<JSONObject>) cmpltOutput.get("columns");
    		}
    		log.info("HeaderList: "+HeaderList);
    		int limit = 0;
    		if(pageNumber == null || pageNumber == 0)
    		{
    			pageNumber = 0;
    		}
    		if(pageSize == null || pageSize == 0)
    		{
    			pageSize = totDataCnt;
    		}
    		limit = ((pageNumber+1) * pageSize + 1)-1;
    		int startIndex=pageNumber*pageSize; 

    		if(limit>totDataCnt){
    			limit=totDataCnt;
    		}

    		log.info("startIndex: "+startIndex+" limit: "+limit);
    		
    		limitedOutputList=outputList.subList(startIndex, limit);
    		finalOutput.put("data", limitedOutputList);
    		if(outputType.equalsIgnoreCase("pivot"))
    			finalOutput.put("data", outputList);
    		finalOutput.put("columns", HeaderList);
    		finalOutput.put("X-COUNT", totDataCnt);
    	}
    	return finalOutput;
    }*/
    
    /**
     * Author: Swetha
     * @param tenantId
     * @param reportId
     * @param filtersMap
     * @return
     * @throws org.json.simple.parser.ParseException
     * @throws IOException
     * @throws OozieClientException
     * @throws URISyntaxException
     */
    @PostMapping("/PivotViewReport")
    @Timed
    public List<LinkedHashMap> PivotViewReportNew(HttpServletRequest request, @RequestParam Long reportId, @RequestBody(required = false) HashMap filtersMap) 
    		throws org.json.simple.parser.ParseException, IOException, OozieClientException, URISyntaxException{
    	HashMap map0=userJdbcService.getuserInfoFromToken(request);
      	Long tenantId=Long.parseLong(map0.get("tenantId").toString());
    	Long userId=9L;
    	log.info("Rest Request to reportingPOC with reportId: "+reportId+" @"+DateTime.now());
    	log.info("filtersMap: "+filtersMap);
    	JSONObject obj = new JSONObject();
    	obj.putAll(filtersMap);
    	log.info("obj: "+obj);
    	
    	List<LinkedHashMap> maps=new ArrayList<LinkedHashMap>();
    	Reports report=reportsRepository.findOne(reportId);
		String reportName=report.getReportName();
		
		ReportRequests repReq=new ReportRequests();
		ReportRequests repReqUpd=new ReportRequests();
		String requestName=report.getReportName()+ZonedDateTime.now();
		repReq.setReqName(requestName);
		repReq.setReportId(reportId);
		repReq.setTenantId(tenantId);
		repReq.setStatus("RUNNING");
			String filMap=obj.toJSONString();
			log.info("filMap: "+filMap);
		repReq.setFilterMap(filMap);
		repReq.setSubmittedTime(ZonedDateTime.now());
		
		repReq.setCreatedBy(userId);
		repReq.setLastUpdatedBy(userId);
		repReq.setCreatedDate(ZonedDateTime.now());
		repReq.setLastUpdatedDate(ZonedDateTime.now());
		repReq.setRequestType("Run");
		repReq=reportRequestsRepository.save(repReq);
    	log.info("repReq :"+repReq);
    	
    	String cmpltFilePath=reportsService.FileWriteHDFS(reportId, obj,"params",tenantId);
    	log.info("done writing file to hdfs");
    	
    	HashMap parameterSet = new HashMap();
		parameterSet.put("param1", reportId);
		parameterSet.put("param2", cmpltFilePath);
		parameterSet.put("param5", "Pivot");
		

		log.info("Api call to Intiate Job for Data Transformation process: "+parameterSet);
		ResponseEntity jobStatus=oozieService.jobIntiateForAcctAndRec(tenantId, userId, "Reporting_Dev", parameterSet,null);
		log.info("jobStatus: "+jobStatus);
		HashMap map=(HashMap) jobStatus.getBody();
		log.info("map: "+map);
		String val=map.get("status").toString();
		log.info("val: "+val);
		JSONObject pivotOutput=new JSONObject();
		LinkedHashMap dataMap=new LinkedHashMap();
		
		String status="";
    	String lastOne="";
		String pivotOutputPath="";
		
		if(val.equalsIgnoreCase("Failed to intiate job")){
			log.info("Reporting Program Failed");
			repReq.setStatus("FAILED");
			repReq.setLastUpdatedDate(ZonedDateTime.now());
			repReq=reportRequestsRepository.save(repReq);
			log.info("updating repReq if it is failed :"+repReq);
		}
		else{
			log.info("Job has been initiated succesfully");
			status=oozieService.getStatusOfOozieJobId(val);
			log.info("status: "+status);
			repReq.setJobId(val);
			repReq.setLastUpdatedDate(ZonedDateTime.now());
			repReq=reportRequestsRepository.save(repReq);
			log.info("updating request with jobId: "+repReq);
			for(int i=0;;i++){
				
				status=oozieService.getStatusOfOozieJobId(val);
				
				if(!(status.equalsIgnoreCase("RUNNING"))){
					log.info("status: "+status);
					
				break;
				//log.info("outputPath at i: "+i+" is: "+outputPath);
					}
				else{
					//log.info("dataMap not retrieved");
				}
			}
			log.info("request to get success job status");
			
			JobActions pivotPathData=jobActionsRepository.findReportPivoutOutputPath(val, tenantId);
			System.out.println("pivotPathData: "+pivotPathData);
			String pivotPath="";
			if(pivotPathData!=null){
				log.info("pivotPathData: "+pivotPathData);
				String actionName=pivotPathData.getActionName();
				String[] actionNamesArr=actionName.split("is: ");
				log.info("actionNamesArr -0: "+actionNamesArr[0]+" actionNamesArr-1: "+actionNamesArr[1]);
				pivotPath=actionNamesArr[1];
				Long schedulerId=pivotPathData.getSchedulerId();
				pivotOutput=reportsService.testFileReading(pivotPath,userId,val,schedulerId,tenantId,reportId);
				JSONObject newPivotFileData = (JSONObject) pivotOutput.clone();
				dataMap.put("pivotOutput", pivotOutput);
				dataMap.put("pivotOutputPath", pivotPath);
				}
			
			if(pivotPath!=null && !(pivotPath.isEmpty()) && pivotPath.length()>1){
				String[] bits = pivotPath.split("/");
				lastOne = bits[bits.length-1];
				String[] pivotFileNameArr=pivotOutputPath.split("/");
				String pivotFileName=pivotFileNameArr[pivotFileNameArr.length-1];
				log.info("file name :"+lastOne);
				status=oozieService.getStatusOfOozieJobId(val);
				log.info("status after processess is completed :");
				repReq.setStatus(status);
				repReq.setGeneratedTime(ZonedDateTime.now());
				repReq.setOutputPath("");
				repReq.setPivotPath(pivotPath);
				repReq.setFileName(lastOne);
				repReq.setLastUpdatedDate(ZonedDateTime.now());
				repReqUpd=reportRequestsRepository.save(repReq);
				log.info(" final repReq :"+repReqUpd);
				
				Notifications notification=new Notifications();
				notification.setModule("REPORTING");
				
				notification.setMessage("Requested "+reportName+" has been generated report");
				notification.setUserId(userId);
				notification.setIsViewed(false);
				notification.setActionType("SCHEDULER");
				SchedulerDetails sch=schedulerDetailsRepository.findByOozieJobId(val);
				notification.setActionValue(sch.getId().toString());
				notification.setTenantId(tenantId);
				notification.setCreatedBy(userId);
				notification.setCreationDate(ZonedDateTime.now());
				notification.setLastUpdatedBy(userId);
				notification.setLastUpdatedDate(ZonedDateTime.now());
				notification=notificationsRepository.save(notification);
				log.info("notification :"+notification);
				
				if(pivotOutput.containsKey("data")){
					log.info("output contains key data");
				}
				else{
					log.info("output doesn't contains key data");
				}
				maps=(List<LinkedHashMap>) pivotOutput.get("data");
				JSONObject requestInfo=(JSONObject) pivotOutput.get("requestInfo");
				log.info("requestInfo from output: "+requestInfo);
				pivotOutput.put("requestInfo", repReq);
				pivotOutput.put("outputPath", pivotPath);
			}
			else{
				log.info("In output path doesnt exists case and Updating Request status");
				status=oozieService.getStatusOfOozieJobId(val);
				log.info("status: "+status);
				repReqUpd.setStatus(status);
				repReqUpd.setLastUpdatedDate(ZonedDateTime.now());
				reportRequestsRepository.save(repReqUpd);
				log.info(" final repReqUpd :"+repReqUpd);
			}
		
			log.info("**end of pivot API** "+ZonedDateTime.now());	
		}
		return maps;
		
    }


    /**
     * Author: Rk
     * Api to present report output pagewise by reading hdfs file
     * @param reportPath
     * @param pageNumber
     * @param pageSize
     * @param response
     * @return
     * @throws IOException
     * @throws URISyntaxException
     * @throws org.json.simple.parser.ParseException
     */
    @PostMapping("/getTrailBalanceReportOutput")
    @Timed
    public JSONObject trailBalanceReport(
    		@RequestParam Long tenantId,@RequestParam Long userId, @RequestParam Long reportId,
			@RequestBody(required = false) HashMap filtersMap,  @RequestParam(required = false) Integer pageNumber,@RequestParam(required = false) Integer pageSize,HttpServletResponse response) throws AnalysisException,
			IOException, ParseException, ClassNotFoundException, SQLException {
    	log.info("I am in Trail Balance Report generation start at: "+new Date());
    	Reports report=reportsRepository.findOne(reportId);
    	Long dvId=report.getSourceViewId();
    	log.info("Filters map : "+filtersMap);
    	
    	// ********************** Preparation work ************************************
    	Map<String,String> dvGroupByCols=new HashMap<String,String>();
    	Map<String,String> sysCols=new HashMap<String,String>();
        String openinigTableSelectCols="", viewSelectCols="", viewName="", selDate="",accRefSeg="", joinCondition=""; // Need to get it dynamically
        
        selDate=(filtersMap.get("dateTimeVal").toString().split("T"))[0];
        viewName=(dataViewsRepository.findOne(dvId).getDataViewName()).toLowerCase();
        Segments naturalSegmentInfo=segmentsRepository.fetchAccQualifySeg(dvId);
        Long coaId=naturalSegmentInfo.getCoaId();
        accRefSeg="accounting_ref_"+naturalSegmentInfo.getSequence().toString();
        
        List<Object[]> dvColNames=reportDefinationRepository.fetchRepDisplayNameAndTemplateLinesColName(reportId,dvId.toString());
        for(Object item[]:dvColNames) {
        	dvGroupByCols.put(item[1].toString(),item[0].toString());
        	openinigTableSelectCols=openinigTableSelectCols+"`openinigTable`.`"+item[1].toString()+"`,";
        	viewSelectCols=viewSelectCols+"`v`.`"+item[1].toString()+"`,";
        	joinCondition=joinCondition+" AND (`openinigTable`.`"+item[1].toString()+"`="+"`additionsTable`.`"+item[1].toString()+"`)";
        }
        Map<String, String> revDvGroupByCols = MapUtils.invertMap(dvGroupByCols);
        List<Object[]> sysColNames=reportDefinationRepository.fetchRepSysColsDisplayNameOriginalColName(reportId);
        for(Object item[]:sysColNames) {
        	sysCols.put(item[0].toString(),item[1].toString());
        }
        Map<String, String> revSysCols = MapUtils.invertMap(sysCols);
        
        List<ReportDefination> columnsDef=reportDefinationRepository.fetchByReportIdOrderByLayoutVal(reportId);
        String aliasSelctCols= "ft."+accRefSeg+" AS "+accRefSeg+", ft.Account_desc AS `Account Description`";
		List<HashMap> columns=new ArrayList<HashMap>();
		HashMap mapA = new HashMap();
		mapA.put("field", accRefSeg);
		mapA.put("width","150px" );
		mapA.put("header", accRefSeg);
		mapA.put("align", "left");
		HashMap mapB = new HashMap();
		mapB.put("field", "Account Description");
		mapB.put("width","200px" );
		mapB.put("header", "Account Description");
		mapB.put("align", "left");
		columns.add(mapA);
		columns.add(mapB);
		for(int i=0;i<columnsDef.size();i++){
			HashMap map = new HashMap();
			if(sysCols.containsKey(columnsDef.get(i).getDisplayName()))
			{
				aliasSelctCols=aliasSelctCols+", ft.`"+sysCols.get(columnsDef.get(i).getDisplayName())+ "` AS `"+columnsDef.get(i).getDisplayName()+"`";
				map.put("width","200px" );
			}
			else if(revDvGroupByCols.containsKey(columnsDef.get(i).getDisplayName())){
				aliasSelctCols=aliasSelctCols+", ft.`"+revDvGroupByCols.get(columnsDef.get(i).getDisplayName())+ "` AS `"+columnsDef.get(i).getDisplayName()+"`";
				map.put("width","150px" );
			}
			map.put("field", columnsDef.get(i).getDisplayName());
			map.put("header", columnsDef.get(i).getDisplayName());
			if(columnsDef.get(i).getDataType()!=null &&columnsDef.get(i).getDataType().equalsIgnoreCase("DECIMAL"))
				map.put("align", "right");
			else
				map.put("align", "left");
			columns.add(map);
		}
        // ********************** End of Preparation work ************************************

    	//*************Add record in to the request list********************/
        List<ReportDefination> reportDefinitionList=reportDefinationRepository.fetchByReportIdOrderByLayoutVal(reportId);
	    List<HashMap> defMapList=new ArrayList<HashMap>();
	    for(int k=0;k<reportDefinitionList.size();k++){
	    	ReportDefination repDef=reportDefinitionList.get(k);
	    	HashMap defMap=new HashMap();
	    	defMap.put("ColumnId", repDef.getId());
	    	defMap.put("layoutDisplayName", repDef.getDisplayName());
	    	defMap.put("columnType", repDef.getRefTypeId());
	    	defMap.put("dataType", repDef.getDataType());
	    	defMapList.add(defMap);
	    }
	    filtersMap.put("outputCols", defMapList);
		ReportRequests repReq=new ReportRequests();
		String requestName=report.getReportName()+ZonedDateTime.now();
		repReq.setReqName(requestName);
		repReq.setReportId(reportId);
		repReq.setTenantId(tenantId);
		repReq.setStatus("RUNNING");
		if(filtersMap!=null)
		{
			JSONObject obj=new JSONObject();
			obj.putAll(filtersMap);
			String map=obj.toJSONString();
		repReq.setFilterMap(map);
		}
		repReq.setSubmittedTime(ZonedDateTime.now());
		repReq.setCreatedBy(userId);
		repReq.setLastUpdatedBy(userId);
		repReq.setCreatedDate(ZonedDateTime.now());
		repReq.setLastUpdatedDate(ZonedDateTime.now());
		repReq.setRequestType("Run");
		repReq=reportRequestsRepository.save(repReq);
    	// ************ Done Add Record in to the request list ******************
    	
    	// ************ Building Conditions query *******************************
    	List<ReportDefination> reportCondList=reportDefinationRepository.fetchReportConditions(reportId);
    	String condQry = "", condOperOrg="",condVal="",colDtType="",colAlias="";
    	
    	for(int i=0;i<reportCondList.size();i++){
    		String conSubQry = "";
    		ReportDefination repCond=reportCondList.get(i);
    		condOperOrg=repCond.getConditionalOperator();
    		condVal=repCond.getConditionalVal();
    		colDtType=repCond.getDataType();
    		if(revDvGroupByCols.containsKey(repCond.getDisplayName())){
    			colAlias="`openinigTable`.`"+revDvGroupByCols.get(repCond.getDisplayName())+"`";
    		}else if(sysCols.containsKey(repCond.getDisplayName()))
    			colAlias="`openinigTable`.`"+sysCols.get(repCond.getDisplayName())+"`";
    		else{
    			log.info("Conditions Col Info not found for :"+repCond.getDisplayName());
    			continue;
    		}
    		if(condQry.length()>0)
    			conSubQry = " AND ";
    		if (colDtType.equalsIgnoreCase("VARCHAR")) {
				if (condOperOrg.equalsIgnoreCase("EQUALS")) {
					conSubQry = conSubQry + colAlias + " = '" + condVal + "'";
				}
				else if (condOperOrg.equalsIgnoreCase("NOT_EQUALS")) {
					conSubQry = conSubQry + colAlias + " <> '" + condVal + "'";
				}
				else if (condOperOrg.equalsIgnoreCase("CONTAINS")) {
					conSubQry = conSubQry + colAlias + " like '%" + condVal + "%'";
				}
				else if (condOperOrg.equalsIgnoreCase("BEGINS_WITH")) {
					conSubQry = conSubQry + colAlias + " like '" + condVal + "%'";
				}
				else if (condOperOrg.equalsIgnoreCase("ENDS_WITH")) {
					conSubQry = conSubQry + colAlias + " like '%" + condVal + "'";
				}
			} else if (colDtType.equalsIgnoreCase("INTEGER")) {
				conSubQry = conSubQry + colAlias + condOperOrg + condVal;
				// handle between
			} else if (colDtType.equalsIgnoreCase("DATE")) {
				// handle between
				conSubQry = conSubQry + colAlias + condOperOrg + "'" + condVal + "'";
			} else if (colDtType.equalsIgnoreCase("DATETIME")) {
				// handle between
				conSubQry = conSubQry + colAlias + condOperOrg + "'" + condVal + "'";
			} else if (colDtType.equalsIgnoreCase("DECIMAL")) {
				// handle between
				conSubQry = conSubQry + colAlias + condOperOrg + condVal;
			}
			condQry = condQry + conSubQry;
    	}
    	log.info("*******final condQry: " + condQry);
    	// ************ End of Building Conditions query *******************************
    	
        
     // ************ Start of Filter Query *******************************
        String filterQuery = "";
		if (filtersMap != null && !(filtersMap.isEmpty())) {
			if (filtersMap.containsKey("fields")) {
				List<HashMap> filtersList = (List<HashMap>) filtersMap
						.get("fields");
				for (int i = 0; i < filtersList.size(); i++) {
					HashMap filterMap = filtersList.get(i);
					String conSubQuery = "";
					String colName = "";
					String selType = filterMap.get("fieldType").toString();
					if (revDvGroupByCols.containsKey(filterMap.get("fieldName"))) {
						colName = " `openinigTable`.`"+revDvGroupByCols.get(filterMap.get("fieldName"))+"`";
					} else if (sysCols.containsKey(filterMap.get("fieldName"))) {
						colName = sysCols.get(filterMap.get("fieldName"));
					} else {
						log.info("Filter parameter info not found for : "
								+ filterMap.get("fieldName"));
						continue;
					}
					if (selType.equalsIgnoreCase("MULTI_SELECT_LOV")
							|| selType.equalsIgnoreCase("SINGLE_SELECT_LOV")
							|| selType.equalsIgnoreCase("SINGLE_SELECTION")
							|| selType.equalsIgnoreCase("TEXT")) {
						String blankQuery = "";
						String selValVar = filterMap.get("selectedValue")
								.toString();
						if (selValVar != null && !(selValVar.isEmpty())
								&& !(selValVar.equalsIgnoreCase("[]"))) {
							selValVar = selValVar.replace("[", "");
							selValVar = selValVar.replace("]", "");
							List<String> selValList = new ArrayList<String>(
									Arrays.asList(selValVar.split(", ")));
							conSubQuery = conSubQuery + colName + " in ('" + String.join("','", selValList)
									+ "') ";
							if (selValList.contains("(Blank)")) {
								conSubQuery = " (" + conSubQuery + " OR " + colName + " is null)";
							}
						}
					} else if (selType.equalsIgnoreCase("AUTO_COMPLETE")) {
						String selVal = filterMap.get("selectedValue")
								.toString();
						if (selVal != null && !(selVal.isEmpty())) {
							conSubQuery = conSubQuery + "`" + colName + "`"
									+ " in ('" + selVal + "') ";
						}
					} else if (selType.equalsIgnoreCase("BOOLEAN_SELECTION")) {
						String selVal = filterMap.get("selectedValue")
								.toString();
						if (selVal != null && !(selVal.isEmpty())) {
							conSubQuery = " `" + colName + "`" + " is "
									+ selVal;
						}
					} else if (selType.equalsIgnoreCase("AMOUNT_RANGE")) {
						String map = filterMap.get("selectedValue").toString();
						if (map != null && !(map.isEmpty())) {
							map = map.replace("{fromValue=", "");
							map = map.replace("toValue=", "");
							String arr[] = map.split(",");
							arr[1] = arr[1].replace("}", "");
							String fromValue = arr[0].trim();
							String toValue = arr[1].trim();
							conSubQuery = " `" + colName + "`" + " between "
									+ fromValue + " and " + toValue;
						}
					}
					if (filterQuery.length() > 0)
						filterQuery = filterQuery + " AND " + conSubQuery;
					else
						filterQuery = filterQuery + conSubQuery;
					log.info("conSubQuery: " + conSubQuery);
				}
			}
			log.info("********final filter Query : " + filterQuery);
		} else {
			log.info("filters doesnt exist");
		}
    	// ************ End of Filters Query *******************************
    	String whereStr="";
    	if(filterQuery.length()>0){
    		whereStr = whereStr + " WHERE "+ filterQuery+" ";
        	if(condQry.length()>0){
        		whereStr = whereStr + " AND "+ condQry+" ";
            }
        }else{
	        if(condQry.length()>0){
	        	whereStr = whereStr + " AND "+ condQry+" ";
	        }
        }
    	
    	// ************ Start of Building Select Query *******************************
        String baseQuery=
        		"SELECT "+aliasSelctCols+
        "		FROM (select "+
        "		openinigTable."+accRefSeg+", "+openinigTableSelectCols+" openinigTable.target_value as Account_desc, "+
        "		IFNULL(sum(openinigTable.Entered_amount),0) as `Opening Entered Amount`, "+
        "	    IFNULL(sum(openinigTable.Accounted_amount),0) as `Opening Accounted Amount`, "+
        "	    IFNULL(sum(additionsTable.Entered_amount),0) as `Additions Entered Amount`, "+
        "	    IFNULL(sum(additionsTable.Accounted_amount),0) as `Additions Accounted Amount`,"+
        "   	IFNULL(sum(postedTable.Entered_amount),0) as `Posted Entered Amount`, "+
        "	    IFNULL(sum(postedTable.Accounted_amount),0) as `Posted Accounted Amount`, "+
        "	    ((IFNULL(sum(openinigTable.Entered_amount),0)+IFNULL(sum(additionsTable.Entered_amount),0))-IFNULL(sum(postedTable.Entered_amount),0)) as `Closing Entered Amount`, "+
        "	    ((IFNULL(sum(openinigTable.Accounted_amount),0)+IFNULL(sum(additionsTable.Accounted_amount),0))-IFNULL(sum(postedTable.Accounted_amount),0)) as `Closing Accounted Amount` "+
        "	 from "+ 
        "	( "+
//        	###### UnPosted (previous day closing balances #####
        "	select "+ 
        
        "	 ad."+accRefSeg+", "+viewSelectCols+" ad.target_value, "+
        "	  sum(ad.amount) as Entered_amount, "+
        "	  sum(ad.accounted_amount) as Accounted_amount "+ 
        "	from "+
        "	  ( "+
        "	    ( "+
        "	      select "+
        "	        asd.original_row_id, "+
        "	        asd.accounting_ref_1, "+
        "	        asd.accounting_ref_2, "+
        "	        asd.accounting_ref_3, "+
        "	        asd.accounting_ref_4, "+
        "	        asd.accounting_ref_5, "+
        "	        asd.accounting_ref_6, "+
        "	        asd.accounting_ref_7, "+
        "	        asd.accounting_ref_8, "+
        "	        asd.accounting_ref_9, "+
        "	        asd.accounting_ref_10, "+
        "	        asd.line_type_detail, "+ 
        "	        asd.category_ref, "+
        "	        asd.original_view_id, "+
        "	        asd.accounted_amount, "+ 
        "	        asd.amount, "+
        "	        cas.target_value "+
        "	      from "+
        "	        ( "+
        "	          SELECT * FROM t_accounting_data "+ 
        "	          where "+
        "	          original_row_id in ( SELECT row_id FROM t_accounted_summary "+ 
        "	              where row_id in "+
        "					(select vi.scrIds from t_accounted_summary summ, "+viewName+" vi where summ.row_id=vi.scrIds and summ.view_id = "+dvId.toString()+" and Date(fileDate) < '"+selDate+"' ) "+ 
        "	                and journal_status is NULL "+
        "	                ) "+
        "	        ) asd, "+
        "	        (SELECT * FROM t_mapping_set_values where mapping_set_id = (select value_id from t_segments where coa_id="+coaId.toString()+" and qualifier='NATURAL_ACCOUNT')) cas "+
        "	        where (asd."+accRefSeg+" = cas.source_value) "+
        "	    ) ad "+
        "	    join (select * from "+viewName+" vi where Date(fileDate) < '"+selDate+"') v "+
        "	  ) "+
        "	where "+
        "	  (ad.original_row_id = v.scrIds) group by "+accRefSeg+", "+viewSelectCols+" target_value "+
        "	) openinigTable "+ 

        "	 LEFT JOIN "+
        "	  ( "+
//        	  ###### Additions #####
        "		SELECT "+
        "		 adt."+accRefSeg+", "+viewSelectCols+
        "		  sum(adt.amount) as Entered_amount, "+
        "		  sum(adt.accounted_amount) as Accounted_amount  FROM t_accounted_summary asum, t_accounting_data adt, "+viewName+" v "+
        "	      where "+
        "				row_id in (SELECT scrIds FROM "+viewName+"  where (  Date(fileDate) <= '"+selDate+"')) "+
        "				and (adt.accounted_summary_id = asum.id) "+
        "	            and (asum.row_id=v.scrIds) "+
        "				and  (asum.created_date between '"+selDate+" 00:00:00' and '"+selDate+" 23:59:59') group by "+viewSelectCols+accRefSeg+
        "	  ) additionsTable "+
        "	  on (openinigTable."+accRefSeg+"=additionsTable."+accRefSeg+joinCondition+") "+
        "	  LEFT JOIN "+
        "	  ( "+
//        	  ###### Posted #####
        "	 SELECT "+
        "	 adt."+accRefSeg+", "+viewSelectCols+
        "	  adt.amount as Entered_amount, "+
        "	  adt.accounted_amount as Accounted_amount  FROM t_accounted_summary asum, t_accounting_data adt, "+viewName+" v where"+ 
        "			row_id in (SELECT scrIds FROM "+viewName+"  where (  Date(fileDate) <= '"+selDate+"'))"+
        "			and (adt.accounted_summary_id = asum.id)"+
        "	        and (asum.journal_status = 'Entered')"+
        "	        and  (asum.created_date between '"+selDate+" 00:00:00' and '"+selDate+" 23:59:59')"+
        "	        and (adt.original_row_id = v.scrIds) "+
        "	  ) postedTable on (openinigTable."+accRefSeg+"=postedTable."+accRefSeg+"  "+joinCondition+")"+
        		whereStr+
        "	  group by "+
        "	  openinigTable."+accRefSeg+", "+openinigTableSelectCols+" openinigTable.target_value) ft";
        
        log.info("********* Final select query :"+baseQuery);
    	// ************ End of Select Query *******************************
    	
    	String dbUrl=env.getProperty("spring.datasource.url");
		String[] parts=dbUrl.split("[\\s@&?$+-]+");
		String host = parts[0].split("/")[2].split(":")[0];
		String schemaName=parts[0].split("/")[3];
		String userName = env.getProperty("spring.datasource.username");
		String password = env.getProperty("spring.datasource.password");
		String jdbcDriver = env.getProperty("spring.datasource.jdbcdriver");
    	   
    	   Connection conn = null;
    	   Statement stmt = null;
    	   Statement stmt2 = null;
		   ResultSet result = null;
		   ResultSet rs=null;
    	   List<LinkedHashMap> mapList2=new ArrayList<LinkedHashMap>();
    	   JSONObject output=new JSONObject();
    	   ReportRequests req=reportRequestsRepository.findOneByTenantIdAndReqName(tenantId,requestName);
		try {
			Class.forName(jdbcDriver);
			conn = DriverManager.getConnection(dbUrl, userName, password);
			log.info("Connected database successfully...");
			stmt = conn.createStatement();
			stmt2 = conn.createStatement();
			String count = null;
			result = stmt2.executeQuery(baseQuery);
			rs = stmt2.getResultSet();
			ResultSetMetaData rsmd2 = result.getMetaData();
			int columnsNumber = rsmd2.getColumnCount();
			int columnCount = rsmd2.getColumnCount();
			while (rs.next()) {
				LinkedHashMap<String, String> map2 = new LinkedHashMap<String, String>();
				for (int i = 1; i <= columnCount; i++) {
					map2.put(rsmd2.getColumnLabel(i), rs.getString(i));
//					if(dvGroupByCols.containsKey(rsmd2.getColumnLabel(i)))
//						map2.put(dvGroupByCols.get(rsmd2.getColumnLabel(i)), rs.getString(i));
//					else if(revSysCols.containsKey(rsmd2.getColumnLabel(i)))
//						map2.put(revSysCols.get(rsmd2.getColumnLabel(i)), rs.getString(i));
//					else
//						map2.put(rsmd2.getColumnLabel(i), rs.getString(i));
				}
				mapList2.add(map2);
			}
			output.put("data", mapList2);
			output.put("requestInfo", req);
			output.put("columns", columns);
			output.put("X-COUNT", mapList2.size());
			response.addIntHeader("X-COUNT", mapList2.size());
			String path = reportsService.FileWriteHDFS(reportId,output,"Output",report.getTenantId());
			req.setStatus("SUCCEEDED");
			req.setGeneratedTime(ZonedDateTime.now());
			req.setOutputPath(path);
			req.setPivotPath(path);
			String[] bits = path.split("/");
			req.setFileName(bits[bits.length-1]);
	    	log.info("repReq :"+req);
		} catch (SQLException se) {
			output.put("data", mapList2);
			req.setStatus("FAILED");
			req.setGeneratedTime(ZonedDateTime.now());
			output.put("requestInfo", req);
			output.put("columns", new ArrayList<HashMap>());
			log.info("se: " + se);
		}
		req=reportRequestsRepository.save(req);
		return output;
    }
    
}
    
